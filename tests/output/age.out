Evaluating KNeighborsClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 5 candidates, totalling 25 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.576776137134 | 0.0421894771594 |  {'n_neighbors': 3} |        2        |
|  0.602212830335 | 0.0681837179748 |  {'n_neighbors': 5} |        1        |
|  0.550089430565 | 0.0156826951236 | {'n_neighbors': 11} |        4        |
|  0.551356410302 | 0.0145692303753 | {'n_neighbors': 21} |        3        |
|  0.545062754313 | 0.0039891285604 | {'n_neighbors': 31} |        5        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.69      0.89      0.78        74
          3       0.00      0.00      0.00         2
         34       0.38      0.17      0.24        35

avg / total       0.58      0.65      0.59       111

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 5 candidates, totalling 25 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.580250505769 | 0.0427000385492 |  {'n_neighbors': 3} |        1        |
|  0.577758854191 |  0.027325739657 |  {'n_neighbors': 5} |        2        |
|  0.553444974511 | 0.0361112777029 | {'n_neighbors': 11} |        3        |
|  0.542943178283 | 0.0031533163039 | {'n_neighbors': 21} |        5        |
|  0.545062754313 | 0.0039891285604 | {'n_neighbors': 31} |        4        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.69      0.78      0.73        74
          3       0.00      0.00      0.00         2
         34       0.37      0.29      0.32        35

avg / total       0.58      0.61      0.59       111

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 5 candidates, totalling 25 fits
Grid scores on validation set:

+-----------------+------------------+---------------------+-----------------+
| test_mean_score |  test_std_score  |        params       | test_rank_score |
+-----------------+------------------+---------------------+-----------------+
|  0.599250577843 | 0.0318021597421  |  {'n_neighbors': 3} |        1        |
|  0.569986638651 | 0.0274929096391  |  {'n_neighbors': 5} |        2        |
|  0.536319316902 | 0.0418786594519  | {'n_neighbors': 11} |        5        |
|  0.558566450746 | 0.0197914449925  | {'n_neighbors': 21} |        3        |
|  0.540013681687 | 0.00496486003403 | {'n_neighbors': 31} |        4        |
+-----------------+------------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.69      0.84      0.76        74
          3       0.00      0.00      0.00         1
         34       0.37      0.21      0.26        34

avg / total       0.58      0.63      0.60       109

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 5 candidates, totalling 25 fits
Grid scores on validation set:

+-----------------+------------------+---------------------+-----------------+
| test_mean_score |  test_std_score  |        params       | test_rank_score |
+-----------------+------------------+---------------------+-----------------+
|  0.610443834497 | 0.0269592810988  |  {'n_neighbors': 3} |        1        |
|  0.56717693093  | 0.0344629701578  |  {'n_neighbors': 5} |        2        |
|  0.566114301227 | 0.0188976856587  | {'n_neighbors': 11} |        3        |
|  0.541315057274 | 0.0158775279839  | {'n_neighbors': 21} |        4        |
|  0.540017568119 | 0.00754363126411 | {'n_neighbors': 31} |        5        |
+-----------------+------------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.69      0.88      0.77        74
          3       0.00      0.00      0.00         1
         34       0.40      0.18      0.24        34

avg / total       0.59      0.65      0.60       109

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 5 candidates, totalling 25 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.58825094756  |  0.035921925161 |  {'n_neighbors': 3} |        1        |
|  0.571403227281 | 0.0326129360621 |  {'n_neighbors': 5} |        2        |
|  0.539932554877 | 0.0193991784366 | {'n_neighbors': 11} |        4        |
|  0.532631114991 | 0.0212867309859 | {'n_neighbors': 21} |        5        |
|  0.545143526692 | 0.0119687769796 | {'n_neighbors': 31} |        3        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      0.72      0.69        74
          3       0.00      0.00      0.00         1
         34       0.30      0.26      0.28        34

avg / total       0.55      0.57      0.56       109

Evaluating RandomForestClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 7 candidates, totalling 35 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.552284263731 |  0.024666688038 | {'n_estimators': 2}  |        7        |
|  0.592812156651 | 0.0297736050601 | {'n_estimators': 3}  |        2        |
|  0.594825674984 | 0.0342080441574 | {'n_estimators': 5}  |        1        |
|  0.583992908793 |  0.039783482125 | {'n_estimators': 10} |        4        |
|  0.589051217459 |  0.028640235709 | {'n_estimators': 20} |        3        |
|  0.578754616193 | 0.0189804861663 | {'n_estimators': 40} |        5        |
|  0.575823223978 |  0.017901291692 | {'n_estimators': 60} |        6        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.71      0.91      0.79        74
          3       0.00      0.00      0.00         2
         34       0.50      0.23      0.31        35

avg / total       0.63      0.68      0.63       111

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 7 candidates, totalling 35 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.557238198682 | 0.0267025554473 | {'n_estimators': 2}  |        3        |
|  0.542298629407 | 0.0284547220457 | {'n_estimators': 3}  |        7        |
|  0.545770503006 | 0.0483973313702 | {'n_estimators': 5}  |        6        |
|  0.558005544989 | 0.0248228714443 | {'n_estimators': 10} |        2        |
|  0.551484167202 | 0.0179549571535 | {'n_estimators': 20} |        5        |
|  0.559776047315 | 0.0246206054827 | {'n_estimators': 40} |        1        |
|  0.551957712184 | 0.0238827946963 | {'n_estimators': 60} |        4        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 40}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.66      0.96      0.78        74
          3       0.00      0.00      0.00         2
         34       0.00      0.00      0.00        35

avg / total       0.44      0.64      0.52       111

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 7 candidates, totalling 35 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.571764512852 | 0.0390431883969 | {'n_estimators': 2}  |        1        |
|  0.529317584758 | 0.0355820221971 | {'n_estimators': 3}  |        7        |
|  0.547441556309 | 0.0286611648089 | {'n_estimators': 5}  |        5        |
|  0.548871797613 | 0.0407055178992 | {'n_estimators': 10} |        4        |
|  0.549006571295 | 0.0181611881782 | {'n_estimators': 20} |        3        |
|  0.557238031887 | 0.0207482199551 | {'n_estimators': 40} |        2        |
|  0.538560572862 | 0.0226241759497 | {'n_estimators': 60} |        6        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 2}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.70      0.92      0.80        74
          3       0.00      0.00      0.00         1
         34       0.55      0.18      0.27        34

avg / total       0.65      0.68      0.62       109

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 7 candidates, totalling 35 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.550604785305 | 0.0124412671946 | {'n_estimators': 2}  |        4        |
|  0.530858396887 | 0.0582598175248 | {'n_estimators': 3}  |        7        |
|  0.575769553025 | 0.0214431847692 | {'n_estimators': 5}  |        1        |
|  0.546586290436 | 0.0466902283535 | {'n_estimators': 10} |        5        |
|  0.573156814903 | 0.0193882731718 | {'n_estimators': 20} |        2        |
|  0.566116624705 | 0.0277090136624 | {'n_estimators': 40} |        3        |
|  0.540479324483 | 0.0165083993025 | {'n_estimators': 60} |        6        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.70      0.82      0.76        74
          3       0.00      0.00      0.00         1
         34       0.38      0.24      0.29        34

avg / total       0.59      0.63      0.61       109

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 7 candidates, totalling 35 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.565797210463 | 0.0194413676213 | {'n_estimators': 2}  |        4        |
|  0.569468132846 | 0.0626693782894 | {'n_estimators': 3}  |        3        |
|  0.573859945862 | 0.0212645664999 | {'n_estimators': 5}  |        2        |
|  0.577336932427 | 0.0351793981573 | {'n_estimators': 10} |        1        |
|  0.550088755015 | 0.0191177361627 | {'n_estimators': 20} |        6        |
|  0.555729550939 | 0.0418342292081 | {'n_estimators': 40} |        5        |
|  0.545497994967 | 0.0169510939155 | {'n_estimators': 60} |        7        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 10}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.70      0.92      0.80        74
          3       0.00      0.00      0.00         1
         34       0.50      0.18      0.26        34

avg / total       0.63      0.68      0.62       109

Evaluating MLPClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 21 candidates, totalling 105 fits
Grid scores on validation set:

+-----------------+------------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score  |                         params                        | test_rank_score |
+-----------------+------------------+-------------------------------------------------------+-----------------+
|  0.545062754313 | 0.0039891285604  |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        12       |
|  0.545062754313 | 0.0039891285604  |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        12       |
|  0.545062754313 | 0.0039891285604  |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        12       |
|  0.545062754313 | 0.0039891285604  |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        12       |
|  0.545062754313 | 0.0039891285604  | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        12       |
|  0.545062754313 | 0.0039891285604  | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        12       |
|  0.545062754313 | 0.0039891285604  | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        12       |
|  0.543955604754 | 0.00375862645724 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        19       |
|   0.5621933768  | 0.0165361590454  |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        3        |
|  0.547022947907 | 0.0145370503787  |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        11       |
|  0.553013934224 | 0.00975809409634 |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        7        |
|  0.539777120059 | 0.0105205187194  |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        21       |
|  0.565819245896 |  0.032666976615  |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        1        |
|  0.56397594348  | 0.0243323356572  |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        2        |
|  0.542833078118 | 0.00473438325304 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        20       |
|  0.547849524686 | 0.0108991883873  |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        10       |
|  0.558001712071 | 0.0146101276151  |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        4        |
|  0.554268981338 |  0.013255197301  |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        6        |
|  0.551011928086 |  0.013503440555  |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        9        |
|  0.55622883816  | 0.0111794967895  |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        5        |
|  0.552226557302 |  0.024705645287  |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        8        |
+-----------------+------------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'tanh', 'hidden_layer_sizes': 120}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      1.00      0.80        74
          3       0.00      0.00      0.00         2
         34       0.00      0.00      0.00        35

avg / total       0.44      0.67      0.53       111

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 21 candidates, totalling 105 fits
Grid scores on validation set:

+-----------------+------------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score  |                         params                        | test_rank_score |
+-----------------+------------------+-------------------------------------------------------+-----------------+
|  0.545062754313 | 0.0039891285604  |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        5        |
|  0.545062754313 | 0.0039891285604  |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        5        |
|  0.545062754313 | 0.0039891285604  |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        5        |
|  0.545062754313 | 0.0039891285604  |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        5        |
|  0.545062754313 | 0.0039891285604  | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        5        |
|  0.545062754313 | 0.0039891285604  | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        5        |
|  0.545062754313 | 0.0039891285604  | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        5        |
|  0.547857166261 | 0.00936336511171 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        4        |
|  0.550624629621 | 0.0124380016723  |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        2        |
|  0.543374472622 | 0.0126402830467  |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        13       |
|  0.541532717235 | 0.0156817367455  |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        16       |
|  0.543253467651 | 0.0124772747379  |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        14       |
|  0.550957709675 | 0.0283631913542  |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        1        |
|  0.539010707752 | 0.0112662981599  |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        18       |
|  0.550216710992 | 0.0124264755251  |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        3        |
|  0.545062754313 | 0.0039891285604  |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        5        |
|  0.542850965739 | 0.00563228966226 |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        15       |
|  0.540635575469 | 0.00230916789457 |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        17       |
|  0.531567415258 | 0.00858787706946 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        21       |
|  0.536355443019 | 0.0118619618398  |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        20       |
|  0.537054921269 | 0.00757486556585 |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        19       |
+-----------------+------------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'tanh', 'hidden_layer_sizes': 120}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      0.99      0.80        74
          3       0.00      0.00      0.00         2
         34       0.50      0.03      0.05        35

avg / total       0.60      0.67      0.55       111

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 21 candidates, totalling 105 fits
Grid scores on validation set:

+-----------------+------------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score  |                         params                        | test_rank_score |
+-----------------+------------------+-------------------------------------------------------+-----------------+
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        13       |
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        13       |
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        13       |
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        13       |
|  0.541115798748 | 0.00592638668239 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        13       |
|  0.541115798748 | 0.00592638668239 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        13       |
|  0.541115798748 | 0.00592638668239 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        13       |
|  0.546923336748 | 0.0159098506461  |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        9        |
|  0.54333437881  | 0.0139288996127  |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        12       |
|  0.538157755824 | 0.0183820941575  |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        21       |
|  0.548170228524 | 0.0154728830499  |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        8        |
|  0.571985648732 | 0.0296405541339  |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        2        |
|  0.553639556421 | 0.0190838391269  |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        6        |
|  0.564854745737 | 0.0298039186442  |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        3        |
|  0.541115798748 | 0.00592638668239 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        13       |
|  0.54514421129  | 0.0146442290549  |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        11       |
|  0.546803682161 | 0.0157760756013  |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        10       |
|  0.549169471019 | 0.0187352427679  |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        7        |
|  0.554274759711 | 0.0138419377574  |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        5        |
|  0.573960250092 | 0.0141371909057  |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        1        |
|  0.557667129416 | 0.0218943461875  |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        4        |
+-----------------+------------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'relu', 'hidden_layer_sizes': 120}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.68      1.00      0.81        74
          3       0.00      0.00      0.00         1
         34       0.00      0.00      0.00        34

avg / total       0.46      0.68      0.55       109

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 21 candidates, totalling 105 fits
Grid scores on validation set:

+-----------------+------------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score  |                         params                        | test_rank_score |
+-----------------+------------------+-------------------------------------------------------+-----------------+
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        13       |
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        13       |
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        13       |
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        13       |
|  0.541115798748 | 0.00592638668239 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        13       |
|  0.541115798748 | 0.00592638668239 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        13       |
|  0.541115798748 | 0.00592638668239 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        13       |
|  0.536698408887 | 0.00670697742077 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        21       |
|  0.546514262557 | 0.0129456327955  |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        9        |
|  0.548454444398 | 0.0126424508433  |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        7        |
|  0.557994104193 | 0.0147635909079  |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        4        |
|  0.561880539889 | 0.0219422146805  |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        3        |
|  0.572194227241 | 0.0188639330211  |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        1        |
|  0.562973765414 | 0.0192967539522  |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        2        |
|  0.541115798748 | 0.00592638668239 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        13       |
|  0.54624175732  | 0.0104980159763  |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        10       |
|  0.55026911675  | 0.0141401458186  |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        6        |
|  0.555248623342 | 0.0157374992768  |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        5        |
|  0.547351348441 | 0.00935799617627 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        8        |
|  0.544224992272 | 0.0139038113402  |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        11       |
|  0.542990199877 | 0.0227833061226  |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        12       |
+-----------------+------------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'tanh', 'hidden_layer_sizes': 120}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.68      1.00      0.81        74
          3       0.00      0.00      0.00         1
         34       0.00      0.00      0.00        34

avg / total       0.46      0.68      0.55       109

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 21 candidates, totalling 105 fits
Grid scores on validation set:

+-----------------+------------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score  |                         params                        | test_rank_score |
+-----------------+------------------+-------------------------------------------------------+-----------------+
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        13       |
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        13       |
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        13       |
|  0.541115798748 | 0.00592638668239 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        13       |
|  0.541115798748 | 0.00592638668239 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        13       |
|  0.541115798748 | 0.00592638668239 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        13       |
|  0.541115798748 | 0.00592638668239 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        13       |
|  0.536701048056 | 0.00817615084034 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        21       |
|  0.554746948787 | 0.0287654742635  |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        5        |
|  0.551607815448 | 0.0263130601552  |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        7        |
|  0.556357982794 | 0.0139159978701  |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        3        |
|  0.544771961672 |  0.026002578386  |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        10       |
|  0.550937110557 | 0.0262503486206  |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        8        |
|  0.570896413924 | 0.0225050464048  |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        1        |
|  0.541115798748 | 0.00592638668239 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        13       |
|  0.543729511312 | 0.00996814658948 |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        11       |
|  0.54339546669  | 0.0168987640719  |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        12       |
|   0.5553057253  | 0.0360182378447  |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        4        |
|  0.560093623835 | 0.0320926481895  |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        2        |
|  0.549220440002 | 0.0250728140204  |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        9        |
|  0.551969420457 | 0.0335653435742  |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        6        |
+-----------------+------------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'tanh', 'hidden_layer_sizes': 150}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.68      1.00      0.81        74
          3       0.00      0.00      0.00         1
         34       0.00      0.00      0.00        34

avg / total       0.46      0.68      0.55       109

Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 8 candidates, totalling 40 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------------+-----------------+
| test_mean_score |  test_std_score |           params           | test_rank_score |
+-----------------+-----------------+----------------------------+-----------------+
|  0.545062754313 | 0.0039891285604 |        {'C': 0.001}        |        4        |
|  0.545062754313 | 0.0039891285604 |        {'C': 0.01}         |        4        |
|  0.545062754313 | 0.0039891285604 | {'C': 0.10000000000000001} |        4        |
|  0.545062754313 | 0.0039891285604 |         {'C': 1.0}         |        4        |
|  0.545062754313 | 0.0039891285604 |        {'C': 10.0}         |        4        |
|  0.571575528445 | 0.0174692612736 |        {'C': 100.0}        |        2        |
|  0.556410099094 | 0.0199949335449 |       {'C': 1000.0}        |        3        |
|  0.572991125141 | 0.0333026578164 |       {'C': 10000.0}       |        1        |
+-----------------+-----------------+----------------------------+-----------------+
Best parameters set found on validation set:

{'C': 10000.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.64      0.76      0.69        74
          3       0.00      0.00      0.00         2
         34       0.24      0.14      0.18        35

avg / total       0.50      0.55      0.52       111

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 8 candidates, totalling 40 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------------+-----------------+
| test_mean_score |  test_std_score |           params           | test_rank_score |
+-----------------+-----------------+----------------------------+-----------------+
|  0.545062754313 | 0.0039891285604 |        {'C': 0.001}        |        5        |
|  0.545062754313 | 0.0039891285604 |        {'C': 0.01}         |        5        |
|  0.545062754313 | 0.0039891285604 | {'C': 0.10000000000000001} |        5        |
|  0.545062754313 | 0.0039891285604 |         {'C': 1.0}         |        5        |
|  0.550216710992 | 0.0124264755251 |        {'C': 10.0}         |        4        |
|  0.555785438687 | 0.0229185741665 |        {'C': 100.0}        |        3        |
|  0.556321334433 |  0.041578454064 |       {'C': 1000.0}        |        2        |
|  0.566860571116 | 0.0143172071788 |       {'C': 10000.0}       |        1        |
+-----------------+-----------------+----------------------------+-----------------+
Best parameters set found on validation set:

{'C': 10000.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.68      0.78      0.73        74
          3       0.00      0.00      0.00         2
         34       0.35      0.26      0.30        35

avg / total       0.56      0.60      0.58       111

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 8 candidates, totalling 40 fits
Grid scores on validation set:

+-----------------+------------------+----------------------------+-----------------+
| test_mean_score |  test_std_score  |           params           | test_rank_score |
+-----------------+------------------+----------------------------+-----------------+
|  0.541115798748 | 0.00592638668239 |        {'C': 0.001}        |        4        |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.01}         |        4        |
|  0.541115798748 | 0.00592638668239 | {'C': 0.10000000000000001} |        4        |
|  0.541115798748 | 0.00592638668239 |         {'C': 1.0}         |        4        |
|  0.545139640259 | 0.0105373112453  |        {'C': 10.0}         |        2        |
|  0.570954226561 | 0.0256045170135  |        {'C': 100.0}        |        1        |
|  0.541998943063 | 0.0428350157386  |       {'C': 1000.0}        |        3        |
|  0.530188365619 | 0.0437499045743  |       {'C': 10000.0}       |        8        |
+-----------------+------------------+----------------------------+-----------------+
Best parameters set found on validation set:

{'C': 100.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.66      0.93      0.78        74
          3       0.00      0.00      0.00         1
         34       0.00      0.00      0.00        34

avg / total       0.45      0.63      0.53       109

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 8 candidates, totalling 40 fits
Grid scores on validation set:

+-----------------+------------------+----------------------------+-----------------+
| test_mean_score |  test_std_score  |           params           | test_rank_score |
+-----------------+------------------+----------------------------+-----------------+
|  0.541115798748 | 0.00592638668239 |        {'C': 0.001}        |        2        |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.01}         |        2        |
|  0.541115798748 | 0.00592638668239 | {'C': 0.10000000000000001} |        2        |
|  0.541115798748 | 0.00592638668239 |         {'C': 1.0}         |        2        |
|  0.541115798748 | 0.00592638668239 |        {'C': 10.0}         |        2        |
|  0.54048010009  | 0.0225168577251  |        {'C': 100.0}        |        7        |
|  0.497752512615 | 0.0295896027966  |       {'C': 1000.0}        |        8        |
|  0.542957891445 | 0.0287948339501  |       {'C': 10000.0}       |        1        |
+-----------------+------------------+----------------------------+-----------------+
Best parameters set found on validation set:

{'C': 10000.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.71      0.78      0.74        74
          3       0.00      0.00      0.00         1
         34       0.36      0.26      0.31        34

avg / total       0.59      0.61      0.60       109

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 8 candidates, totalling 40 fits
Grid scores on validation set:

+-----------------+------------------+----------------------------+-----------------+
| test_mean_score |  test_std_score  |           params           | test_rank_score |
+-----------------+------------------+----------------------------+-----------------+
|  0.541115798748 | 0.00592638668239 |        {'C': 0.001}        |        5        |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.01}         |        5        |
|  0.541115798748 | 0.00592638668239 | {'C': 0.10000000000000001} |        5        |
|  0.541115798748 | 0.00592638668239 |         {'C': 1.0}         |        5        |
|  0.544022216017 |  0.011042473139  |        {'C': 10.0}         |        4        |
|  0.56209470365  | 0.0238499334184  |        {'C': 100.0}        |        2        |
|  0.571685006745 |  0.039186698723  |       {'C': 1000.0}        |        1        |
|   0.5556738548  | 0.0578415871398  |       {'C': 10000.0}       |        3        |
+-----------------+------------------+----------------------------+-----------------+
Best parameters set found on validation set:

{'C': 1000.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      0.85      0.75        74
          3       0.00      0.00      0.00         1
         34       0.27      0.12      0.16        34

avg / total       0.54      0.61      0.56       109

Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 64 candidates, totalling 320 fits
Grid scores on validation set:

+-----------------+------------------+----------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score  |                          params                          | test_rank_score |
+-----------------+------------------+----------------------------------------------------------+-----------------+
|  0.545062754313 | 0.0039891285604  |               {'C': 0.001, 'gamma': 0.001}               |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.001, 'gamma': 0.01}                |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.001, 'gamma': 0.10000000000000001}        |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 0.001, 'gamma': 1.0}                |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.001, 'gamma': 10.0}                |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.001, 'gamma': 100.0}               |        25       |
|  0.545062754313 | 0.0039891285604  |              {'C': 0.001, 'gamma': 1000.0}               |        25       |
|  0.545062754313 | 0.0039891285604  |              {'C': 0.001, 'gamma': 10000.0}              |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.01, 'gamma': 0.001}                |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 0.01, 'gamma': 0.01}                |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.01, 'gamma': 0.10000000000000001}         |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 0.01, 'gamma': 1.0}                 |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 0.01, 'gamma': 10.0}                |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.01, 'gamma': 100.0}                |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.01, 'gamma': 1000.0}               |        25       |
|  0.545062754313 | 0.0039891285604  |              {'C': 0.01, 'gamma': 10000.0}               |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.10000000000000001, 'gamma': 0.001}        |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.10000000000000001, 'gamma': 0.01}         |        25       |
|  0.545062754313 | 0.0039891285604  | {'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        25       |
|  0.545062754313 | 0.0039891285604  |         {'C': 0.10000000000000001, 'gamma': 1.0}         |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.10000000000000001, 'gamma': 10.0}         |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.10000000000000001, 'gamma': 100.0}        |        25       |
|  0.545062754313 | 0.0039891285604  |       {'C': 0.10000000000000001, 'gamma': 1000.0}        |        25       |
|  0.545062754313 | 0.0039891285604  |       {'C': 0.10000000000000001, 'gamma': 10000.0}       |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 1.0, 'gamma': 0.001}                |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 1.0, 'gamma': 0.01}                 |        25       |
|  0.545062754313 | 0.0039891285604  |         {'C': 1.0, 'gamma': 0.10000000000000001}         |        25       |
|  0.545062754313 | 0.0039891285604  |                 {'C': 1.0, 'gamma': 1.0}                 |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 1.0, 'gamma': 10.0}                 |        25       |
|  0.550216944908 | 0.0099126563118  |                {'C': 1.0, 'gamma': 100.0}                |        20       |
|  0.545062754313 | 0.0039891285604  |               {'C': 1.0, 'gamma': 1000.0}                |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 1.0, 'gamma': 10000.0}               |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 10.0, 'gamma': 0.001}                |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 10.0, 'gamma': 0.01}                |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 10.0, 'gamma': 0.10000000000000001}         |        25       |
|  0.552029250014 |  0.014301644804  |                {'C': 10.0, 'gamma': 1.0}                 |        17       |
|  0.586791671724 | 0.0382728099707  |                {'C': 10.0, 'gamma': 10.0}                |        2        |
|  0.550118136425 | 0.0233438228286  |               {'C': 10.0, 'gamma': 100.0}                |        21       |
|  0.555790403062 | 0.0114250682405  |               {'C': 10.0, 'gamma': 1000.0}               |        12       |
|  0.554939025969 | 0.0136313228769  |              {'C': 10.0, 'gamma': 10000.0}               |        14       |
|  0.545062754313 | 0.0039891285604  |               {'C': 100.0, 'gamma': 0.001}               |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 100.0, 'gamma': 0.01}                |        25       |
|  0.551743362027 | 0.00962449567887 |        {'C': 100.0, 'gamma': 0.10000000000000001}        |        19       |
|  0.584277033564 |  0.037753408592  |                {'C': 100.0, 'gamma': 1.0}                |        3        |
|  0.547745619061 |  0.030065642607  |               {'C': 100.0, 'gamma': 10.0}                |        24       |
|  0.55433250469  | 0.0238491281967  |               {'C': 100.0, 'gamma': 100.0}               |        15       |
|  0.558444004184 | 0.0215278990097  |              {'C': 100.0, 'gamma': 1000.0}               |        9        |
|  0.554109211532 | 0.0141098590321  |              {'C': 100.0, 'gamma': 10000.0}              |        16       |
|  0.545062754313 | 0.0039891285604  |              {'C': 1000.0, 'gamma': 0.001}               |        25       |
|  0.551744814238 | 0.0115802453622  |               {'C': 1000.0, 'gamma': 0.01}               |        18       |
|  0.542831588457 | 0.0206200460702  |       {'C': 1000.0, 'gamma': 0.10000000000000001}        |        62       |
|  0.521397056514 | 0.0106071385085  |               {'C': 1000.0, 'gamma': 1.0}                |        64       |
|  0.531251763825 | 0.0335029782781  |               {'C': 1000.0, 'gamma': 10.0}               |        63       |
|  0.601031637701 | 0.0316114135094  |              {'C': 1000.0, 'gamma': 100.0}               |        1        |
|  0.562383033826 | 0.0198710673005  |              {'C': 1000.0, 'gamma': 1000.0}              |        7        |
|  0.547987034797 | 0.0135610412994  |             {'C': 1000.0, 'gamma': 10000.0}              |        23       |
|  0.549110980825 | 0.0132397782755  |              {'C': 10000.0, 'gamma': 0.001}              |        22       |
|  0.555877752456 | 0.0151339810044  |              {'C': 10000.0, 'gamma': 0.01}               |        11       |
|  0.579523433628 | 0.0420305719226  |       {'C': 10000.0, 'gamma': 0.10000000000000001}       |        4        |
|   0.5571756435  | 0.0607931811817  |               {'C': 10000.0, 'gamma': 1.0}               |        10       |
|  0.555263716077 | 0.0492542601272  |              {'C': 10000.0, 'gamma': 10.0}               |        13       |
|  0.574749512472 | 0.0360904354799  |              {'C': 10000.0, 'gamma': 100.0}              |        5        |
|  0.563004303352 |  0.017934606245  |             {'C': 10000.0, 'gamma': 1000.0}              |        6        |
|  0.558976833174 | 0.0155608741402  |             {'C': 10000.0, 'gamma': 10000.0}             |        8        |
+-----------------+------------------+----------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'C': 1000.0, 'gamma': 100.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      0.93      0.78        74
          3       0.00      0.00      0.00         2
         34       0.38      0.09      0.14        35

avg / total       0.56      0.65      0.56       111

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 64 candidates, totalling 320 fits
Grid scores on validation set:

+-----------------+------------------+----------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score  |                          params                          | test_rank_score |
+-----------------+------------------+----------------------------------------------------------+-----------------+
|  0.545062754313 | 0.0039891285604  |               {'C': 0.001, 'gamma': 0.001}               |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.001, 'gamma': 0.01}                |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.001, 'gamma': 0.10000000000000001}        |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 0.001, 'gamma': 1.0}                |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.001, 'gamma': 10.0}                |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.001, 'gamma': 100.0}               |        25       |
|  0.545062754313 | 0.0039891285604  |              {'C': 0.001, 'gamma': 1000.0}               |        25       |
|  0.545062754313 | 0.0039891285604  |              {'C': 0.001, 'gamma': 10000.0}              |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.01, 'gamma': 0.001}                |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 0.01, 'gamma': 0.01}                |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.01, 'gamma': 0.10000000000000001}         |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 0.01, 'gamma': 1.0}                 |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 0.01, 'gamma': 10.0}                |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.01, 'gamma': 100.0}                |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 0.01, 'gamma': 1000.0}               |        25       |
|  0.545062754313 | 0.0039891285604  |              {'C': 0.01, 'gamma': 10000.0}               |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.10000000000000001, 'gamma': 0.001}        |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.10000000000000001, 'gamma': 0.01}         |        25       |
|  0.545062754313 | 0.0039891285604  | {'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        25       |
|  0.545062754313 | 0.0039891285604  |         {'C': 0.10000000000000001, 'gamma': 1.0}         |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.10000000000000001, 'gamma': 10.0}         |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 0.10000000000000001, 'gamma': 100.0}        |        25       |
|  0.545062754313 | 0.0039891285604  |       {'C': 0.10000000000000001, 'gamma': 1000.0}        |        25       |
|  0.545062754313 | 0.0039891285604  |       {'C': 0.10000000000000001, 'gamma': 10000.0}       |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 1.0, 'gamma': 0.001}                |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 1.0, 'gamma': 0.01}                 |        25       |
|  0.545062754313 | 0.0039891285604  |         {'C': 1.0, 'gamma': 0.10000000000000001}         |        25       |
|  0.545062754313 | 0.0039891285604  |                 {'C': 1.0, 'gamma': 1.0}                 |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 1.0, 'gamma': 10.0}                 |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 1.0, 'gamma': 100.0}                |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 1.0, 'gamma': 1000.0}                |        25       |
|  0.543955604754 | 0.00375862645724 |               {'C': 1.0, 'gamma': 10000.0}               |        63       |
|  0.545062754313 | 0.0039891285604  |               {'C': 10.0, 'gamma': 0.001}                |        25       |
|  0.545062754313 | 0.0039891285604  |                {'C': 10.0, 'gamma': 0.01}                |        25       |
|  0.545062754313 | 0.0039891285604  |        {'C': 10.0, 'gamma': 0.10000000000000001}         |        25       |
|  0.570246299458 | 0.0249448320838  |                {'C': 10.0, 'gamma': 1.0}                 |        11       |
|  0.591577600835 | 0.0367501128127  |                {'C': 10.0, 'gamma': 10.0}                |        5        |
|  0.558987343768 | 0.0232946453154  |               {'C': 10.0, 'gamma': 100.0}                |        17       |
|  0.544203055338 | 0.00730067370796 |               {'C': 10.0, 'gamma': 1000.0}               |        62       |
|  0.542848455195 | 0.00314481139763 |              {'C': 10.0, 'gamma': 10000.0}               |        64       |
|  0.545062754313 | 0.0039891285604  |               {'C': 100.0, 'gamma': 0.001}               |        25       |
|  0.545062754313 | 0.0039891285604  |               {'C': 100.0, 'gamma': 0.01}                |        25       |
|  0.556909653209 | 0.0233965234686  |        {'C': 100.0, 'gamma': 0.10000000000000001}        |        22       |
|  0.597299389637 | 0.0462809757418  |                {'C': 100.0, 'gamma': 1.0}                |        3        |
|  0.595097469647 | 0.0357197028861  |               {'C': 100.0, 'gamma': 10.0}                |        4        |
|  0.563687736663 | 0.0352591164273  |               {'C': 100.0, 'gamma': 100.0}               |        14       |
|  0.557600143514 | 0.0238866120274  |              {'C': 100.0, 'gamma': 1000.0}               |        19       |
|  0.556911709604 | 0.0202900130747  |              {'C': 100.0, 'gamma': 10000.0}              |        21       |
|  0.545062754313 | 0.0039891285604  |              {'C': 1000.0, 'gamma': 0.001}               |        25       |
|  0.564417358805 | 0.0134278256248  |               {'C': 1000.0, 'gamma': 0.01}               |        13       |
|  0.588022944865 | 0.0446067447539  |       {'C': 1000.0, 'gamma': 0.10000000000000001}        |        6        |
|  0.601530560362 | 0.0507040079844  |               {'C': 1000.0, 'gamma': 1.0}                |        2        |
|  0.602209020019 | 0.0230302473185  |               {'C': 1000.0, 'gamma': 10.0}               |        1        |
|  0.559142895165 |  0.022918890951  |              {'C': 1000.0, 'gamma': 100.0}               |        16       |
|  0.561222979993 | 0.0233836540777  |              {'C': 1000.0, 'gamma': 1000.0}              |        15       |
|  0.557740010716 | 0.0164855599461  |             {'C': 1000.0, 'gamma': 10000.0}              |        18       |
|  0.568016739969 | 0.0146802483652  |              {'C': 10000.0, 'gamma': 0.001}              |        12       |
|  0.574648376268 | 0.0256687142561  |              {'C': 10000.0, 'gamma': 0.01}               |        9        |
|  0.551375778893 | 0.0183211391656  |       {'C': 10000.0, 'gamma': 0.10000000000000001}       |        23       |
|  0.587081251241 | 0.0263075108812  |               {'C': 10000.0, 'gamma': 1.0}               |        7        |
|  0.57233660266  | 0.0426229595795  |              {'C': 10000.0, 'gamma': 10.0}               |        10       |
|  0.582175738228 | 0.0375747538703  |              {'C': 10000.0, 'gamma': 100.0}              |        8        |
|  0.55698916881  | 0.0169206722144  |             {'C': 10000.0, 'gamma': 1000.0}              |        20       |
|  0.548003831266 | 0.0135081397443  |             {'C': 10000.0, 'gamma': 10000.0}             |        24       |
+-----------------+------------------+----------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'C': 1000.0, 'gamma': 10.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.65      0.62      0.63        74
          3       0.00      0.00      0.00         2
         34       0.28      0.31      0.29        35

avg / total       0.52      0.51      0.52       111

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 64 candidates, totalling 320 fits
Grid scores on validation set:

+-----------------+------------------+----------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score  |                          params                          | test_rank_score |
+-----------------+------------------+----------------------------------------------------------+-----------------+
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 0.001}               |        27       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 0.01}                |        27       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.001, 'gamma': 0.10000000000000001}        |        27       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.001, 'gamma': 1.0}                |        27       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 10.0}                |        27       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 100.0}               |        27       |
|  0.541115798748 | 0.00592638668239 |              {'C': 0.001, 'gamma': 1000.0}               |        27       |
|  0.541115798748 | 0.00592638668239 |              {'C': 0.001, 'gamma': 10000.0}              |        27       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.01, 'gamma': 0.001}                |        27       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.01, 'gamma': 0.01}                |        27       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.01, 'gamma': 0.10000000000000001}         |        27       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.01, 'gamma': 1.0}                 |        27       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.01, 'gamma': 10.0}                |        27       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.01, 'gamma': 100.0}                |        27       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.01, 'gamma': 1000.0}               |        27       |
|  0.541115798748 | 0.00592638668239 |              {'C': 0.01, 'gamma': 10000.0}               |        27       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 0.001}        |        27       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 0.01}         |        27       |
|  0.541115798748 | 0.00592638668239 | {'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        27       |
|  0.541115798748 | 0.00592638668239 |         {'C': 0.10000000000000001, 'gamma': 1.0}         |        27       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 10.0}         |        27       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 100.0}        |        27       |
|  0.541115798748 | 0.00592638668239 |       {'C': 0.10000000000000001, 'gamma': 1000.0}        |        27       |
|  0.541115798748 | 0.00592638668239 |       {'C': 0.10000000000000001, 'gamma': 10000.0}       |        27       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 0.001}                |        27       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 0.01}                 |        27       |
|  0.541115798748 | 0.00592638668239 |         {'C': 1.0, 'gamma': 0.10000000000000001}         |        27       |
|  0.541115798748 | 0.00592638668239 |                 {'C': 1.0, 'gamma': 1.0}                 |        27       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 10.0}                 |        27       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 100.0}                |        27       |
|  0.541115798748 | 0.00592638668239 |               {'C': 1.0, 'gamma': 1000.0}                |        27       |
|  0.541115798748 | 0.00592638668239 |               {'C': 1.0, 'gamma': 10000.0}               |        27       |
|  0.541115798748 | 0.00592638668239 |               {'C': 10.0, 'gamma': 0.001}                |        27       |
|  0.541115798748 | 0.00592638668239 |                {'C': 10.0, 'gamma': 0.01}                |        27       |
|  0.541115798748 | 0.00592638668239 |        {'C': 10.0, 'gamma': 0.10000000000000001}         |        27       |
|  0.559232385772 | 0.0250951590911  |                {'C': 10.0, 'gamma': 1.0}                 |        13       |
|  0.558758423561 | 0.0270042925653  |                {'C': 10.0, 'gamma': 10.0}                |        14       |
|  0.559953645789 | 0.0236353778321  |               {'C': 10.0, 'gamma': 100.0}                |        12       |
|  0.541648678566 | 0.0127604430932  |               {'C': 10.0, 'gamma': 1000.0}               |        26       |
|  0.546246328351 | 0.0146156347844  |              {'C': 10.0, 'gamma': 10000.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 100.0, 'gamma': 0.001}               |        27       |
|  0.541115798748 | 0.00592638668239 |               {'C': 100.0, 'gamma': 0.01}                |        27       |
|  0.562589419713 | 0.0171805430257  |        {'C': 100.0, 'gamma': 0.10000000000000001}        |        11       |
|  0.607693148031 | 0.0347922984689  |                {'C': 100.0, 'gamma': 1.0}                |        1        |
|  0.563846956233 | 0.0444238348321  |               {'C': 100.0, 'gamma': 10.0}                |        10       |
|  0.567005808588 | 0.0273363703807  |               {'C': 100.0, 'gamma': 100.0}               |        8        |
|  0.546511763425 | 0.0120778355152  |              {'C': 100.0, 'gamma': 1000.0}               |        22       |
|  0.548596199803 | 0.0219968104136  |              {'C': 100.0, 'gamma': 10000.0}              |        20       |
|  0.541115798748 | 0.00592638668239 |              {'C': 1000.0, 'gamma': 0.001}               |        27       |
|  0.564785930513 | 0.0181950310682  |               {'C': 1000.0, 'gamma': 0.01}               |        9        |
|  0.58560291754  | 0.0365158583645  |       {'C': 1000.0, 'gamma': 0.10000000000000001}        |        3        |
|  0.54614226452  | 0.0316873312118  |               {'C': 1000.0, 'gamma': 1.0}                |        24       |
|  0.577129917593 | 0.0395650022608  |               {'C': 1000.0, 'gamma': 10.0}               |        4        |
|  0.574574628302 |  0.012647997117  |              {'C': 1000.0, 'gamma': 100.0}               |        6        |
|  0.544484519235 | 0.0133483202207  |              {'C': 1000.0, 'gamma': 1000.0}              |        25       |
|  0.551791373371 | 0.0216500863176  |             {'C': 1000.0, 'gamma': 10000.0}              |        19       |
|  0.557868971266 | 0.0231267074759  |              {'C': 10000.0, 'gamma': 0.001}              |        15       |
|  0.557212430902 | 0.0175060056209  |              {'C': 10000.0, 'gamma': 0.01}               |        16       |
|   0.5755745236  | 0.0279890837813  |       {'C': 10000.0, 'gamma': 0.10000000000000001}       |        5        |
|  0.556677913815 | 0.0381917120991  |               {'C': 10000.0, 'gamma': 1.0}               |        17       |
|  0.572708826462 | 0.0531773864005  |              {'C': 10000.0, 'gamma': 10.0}               |        7        |
|  0.592971881147 | 0.0232041949756  |              {'C': 10000.0, 'gamma': 100.0}              |        2        |
|  0.546633700356 | 0.0156972561861  |             {'C': 10000.0, 'gamma': 1000.0}              |        21       |
|  0.554137815232 |  0.012568668541  |             {'C': 10000.0, 'gamma': 10000.0}             |        18       |
+-----------------+------------------+----------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'C': 100.0, 'gamma': 1.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.69      0.89      0.78        74
          3       0.00      0.00      0.00         1
         34       0.43      0.18      0.25        34

avg / total       0.61      0.66      0.61       109

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 64 candidates, totalling 320 fits
Grid scores on validation set:

+-----------------+------------------+----------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score  |                          params                          | test_rank_score |
+-----------------+------------------+----------------------------------------------------------+-----------------+
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 0.001}               |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 0.01}                |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.001, 'gamma': 0.10000000000000001}        |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.001, 'gamma': 1.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 10.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 100.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |              {'C': 0.001, 'gamma': 1000.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |              {'C': 0.001, 'gamma': 10000.0}              |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.01, 'gamma': 0.001}                |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.01, 'gamma': 0.01}                |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.01, 'gamma': 0.10000000000000001}         |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.01, 'gamma': 1.0}                 |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.01, 'gamma': 10.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.01, 'gamma': 100.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.01, 'gamma': 1000.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |              {'C': 0.01, 'gamma': 10000.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 0.001}        |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 0.01}         |        23       |
|  0.541115798748 | 0.00592638668239 | {'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        23       |
|  0.541115798748 | 0.00592638668239 |         {'C': 0.10000000000000001, 'gamma': 1.0}         |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 10.0}         |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 100.0}        |        23       |
|  0.541115798748 | 0.00592638668239 |       {'C': 0.10000000000000001, 'gamma': 1000.0}        |        23       |
|  0.541115798748 | 0.00592638668239 |       {'C': 0.10000000000000001, 'gamma': 10000.0}       |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 0.001}                |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 0.01}                 |        23       |
|  0.541115798748 | 0.00592638668239 |         {'C': 1.0, 'gamma': 0.10000000000000001}         |        23       |
|  0.541115798748 | 0.00592638668239 |                 {'C': 1.0, 'gamma': 1.0}                 |        23       |
|  0.540016180819 | 0.0068115792607  |                {'C': 1.0, 'gamma': 10.0}                 |        62       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 100.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 1.0, 'gamma': 1000.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 1.0, 'gamma': 10000.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 10.0, 'gamma': 0.001}                |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 10.0, 'gamma': 0.01}                |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 10.0, 'gamma': 0.10000000000000001}         |        23       |
|  0.544034932243 | 0.00885012296274 |                {'C': 10.0, 'gamma': 1.0}                 |        20       |
|  0.562853397483 | 0.0325069574094  |                {'C': 10.0, 'gamma': 10.0}                |        7        |
|  0.55345813042  | 0.0283063246132  |               {'C': 10.0, 'gamma': 100.0}                |        11       |
|  0.554837548319 |  0.019245553303  |               {'C': 10.0, 'gamma': 1000.0}               |        10       |
|  0.544989672622 | 0.00576069806407 |              {'C': 10.0, 'gamma': 10000.0}               |        17       |
|  0.541115798748 | 0.00592638668239 |               {'C': 100.0, 'gamma': 0.001}               |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 100.0, 'gamma': 0.01}                |        23       |
|  0.54624175732  | 0.0104980159763  |        {'C': 100.0, 'gamma': 0.10000000000000001}        |        15       |
|  0.549781886223 | 0.0564044275155  |                {'C': 100.0, 'gamma': 1.0}                |        14       |
|  0.555929690597 | 0.0410321119632  |               {'C': 100.0, 'gamma': 10.0}                |        8        |
|  0.576501140534 | 0.0321736365044  |               {'C': 100.0, 'gamma': 100.0}               |        5        |
|  0.555139417586 | 0.0296461149899  |              {'C': 100.0, 'gamma': 1000.0}               |        9        |
|  0.54404002233  | 0.0114525044308  |              {'C': 100.0, 'gamma': 10000.0}              |        19       |
|  0.541115798748 | 0.00592638668239 |              {'C': 1000.0, 'gamma': 0.001}               |        23       |
|  0.551367196836 | 0.0135404859044  |               {'C': 1000.0, 'gamma': 0.01}               |        12       |
|  0.54085236547  |  0.023027577234  |       {'C': 1000.0, 'gamma': 0.10000000000000001}        |        60       |
|  0.523649115725 | 0.0311976834017  |               {'C': 1000.0, 'gamma': 1.0}                |        64       |
|  0.598475977347 | 0.0276151569705  |               {'C': 1000.0, 'gamma': 10.0}               |        1        |
|  0.582328577731 | 0.0326828411335  |              {'C': 1000.0, 'gamma': 100.0}               |        3        |
|  0.550532011297 |  0.022129863649  |              {'C': 1000.0, 'gamma': 1000.0}              |        13       |
|  0.542773808465 | 0.00887380398966 |             {'C': 1000.0, 'gamma': 10000.0}              |        22       |
|  0.544048479793 | 0.0163344308931  |              {'C': 10000.0, 'gamma': 0.001}              |        18       |
|  0.545673686343 | 0.0220301943337  |              {'C': 10000.0, 'gamma': 0.01}               |        16       |
|  0.564602184156 | 0.0289783300768  |       {'C': 10000.0, 'gamma': 0.10000000000000001}       |        6        |
|  0.533865233383 | 0.0520391246538  |               {'C': 10000.0, 'gamma': 1.0}               |        63       |
|  0.596724727915 |  0.022281483168  |              {'C': 10000.0, 'gamma': 10.0}               |        2        |
|  0.57841870641  | 0.0345630672239  |              {'C': 10000.0, 'gamma': 100.0}              |        4        |
|  0.540521933946 | 0.0114700492151  |             {'C': 10000.0, 'gamma': 1000.0}              |        61       |
|  0.542941791702 | 0.0126240688798  |             {'C': 10000.0, 'gamma': 10000.0}             |        21       |
+-----------------+------------------+----------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'C': 1000.0, 'gamma': 10.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      0.64      0.65        74
          3       0.00      0.00      0.00         1
         34       0.28      0.32      0.30        34

avg / total       0.54      0.53      0.54       109

# Tuning hyper-parameters for f1_weighted

Fitting 5 folds for each of 64 candidates, totalling 320 fits
Grid scores on validation set:

+-----------------+------------------+----------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score  |                          params                          | test_rank_score |
+-----------------+------------------+----------------------------------------------------------+-----------------+
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 0.001}               |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 0.01}                |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.001, 'gamma': 0.10000000000000001}        |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.001, 'gamma': 1.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 10.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.001, 'gamma': 100.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |              {'C': 0.001, 'gamma': 1000.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |              {'C': 0.001, 'gamma': 10000.0}              |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.01, 'gamma': 0.001}                |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.01, 'gamma': 0.01}                |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.01, 'gamma': 0.10000000000000001}         |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.01, 'gamma': 1.0}                 |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 0.01, 'gamma': 10.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.01, 'gamma': 100.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 0.01, 'gamma': 1000.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |              {'C': 0.01, 'gamma': 10000.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 0.001}        |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 0.01}         |        23       |
|  0.541115798748 | 0.00592638668239 | {'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        23       |
|  0.541115798748 | 0.00592638668239 |         {'C': 0.10000000000000001, 'gamma': 1.0}         |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 10.0}         |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 0.10000000000000001, 'gamma': 100.0}        |        23       |
|  0.541115798748 | 0.00592638668239 |       {'C': 0.10000000000000001, 'gamma': 1000.0}        |        23       |
|  0.541115798748 | 0.00592638668239 |       {'C': 0.10000000000000001, 'gamma': 10000.0}       |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 0.001}                |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 0.01}                 |        23       |
|  0.541115798748 | 0.00592638668239 |         {'C': 1.0, 'gamma': 0.10000000000000001}         |        23       |
|  0.541115798748 | 0.00592638668239 |                 {'C': 1.0, 'gamma': 1.0}                 |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 10.0}                 |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 1.0, 'gamma': 100.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 1.0, 'gamma': 1000.0}                |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 1.0, 'gamma': 10000.0}               |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 10.0, 'gamma': 0.001}                |        23       |
|  0.541115798748 | 0.00592638668239 |                {'C': 10.0, 'gamma': 0.01}                |        23       |
|  0.541115798748 | 0.00592638668239 |        {'C': 10.0, 'gamma': 0.10000000000000001}         |        23       |
|  0.565143616687 | 0.0218370220924  |                {'C': 10.0, 'gamma': 1.0}                 |        12       |
|  0.556123325316 | 0.0421515359553  |                {'C': 10.0, 'gamma': 10.0}                |        16       |
|  0.57472294112  | 0.0444708965376  |               {'C': 10.0, 'gamma': 100.0}                |        6        |
|  0.555098690218 | 0.0161736246484  |               {'C': 10.0, 'gamma': 1000.0}               |        17       |
|  0.538914063758 | 0.00578805431142 |              {'C': 10.0, 'gamma': 10000.0}               |        62       |
|  0.541115798748 | 0.00592638668239 |               {'C': 100.0, 'gamma': 0.001}               |        23       |
|  0.541115798748 | 0.00592638668239 |               {'C': 100.0, 'gamma': 0.01}                |        23       |
|  0.540571035497 | 0.00817711410682 |        {'C': 100.0, 'gamma': 0.10000000000000001}        |        61       |
|  0.561517958695 | 0.0206301260363  |                {'C': 100.0, 'gamma': 1.0}                |        14       |
|  0.575449936189 |  0.064371129318  |               {'C': 100.0, 'gamma': 10.0}                |        5        |
|  0.576683458057 | 0.0391623496902  |               {'C': 100.0, 'gamma': 100.0}               |        4        |
|   0.5640558709  | 0.0261384435057  |              {'C': 100.0, 'gamma': 1000.0}               |        13       |
|  0.552074409152 | 0.00998701355229 |              {'C': 100.0, 'gamma': 10000.0}              |        18       |
|  0.541115798748 | 0.00592638668239 |              {'C': 1000.0, 'gamma': 0.001}               |        23       |
|  0.538901395745 | 0.00822818470025 |               {'C': 1000.0, 'gamma': 0.01}               |        63       |
|  0.565463768235 | 0.0256520279117  |       {'C': 1000.0, 'gamma': 0.10000000000000001}        |        11       |
|  0.584868644409 |  0.050330553864  |               {'C': 1000.0, 'gamma': 1.0}                |        2        |
|  0.56784404992  | 0.0390368351454  |               {'C': 1000.0, 'gamma': 10.0}               |        9        |
|  0.603899996143 | 0.0449781669786  |              {'C': 1000.0, 'gamma': 100.0}               |        1        |
|  0.569690071395 | 0.0210634255488  |              {'C': 1000.0, 'gamma': 1000.0}              |        8        |
|  0.559130489755 |  0.022664383603  |             {'C': 1000.0, 'gamma': 10000.0}              |        15       |
|  0.544022216017 |  0.011042473139  |              {'C': 10000.0, 'gamma': 0.001}              |        21       |
|   0.5467744437  | 0.0311880155578  |              {'C': 10000.0, 'gamma': 0.01}               |        20       |
|  0.538372666944 | 0.0424865279845  |       {'C': 10000.0, 'gamma': 0.10000000000000001}       |        64       |
|  0.54319315606  | 0.0638065888746  |               {'C': 10000.0, 'gamma': 1.0}               |        22       |
|  0.571204587119 | 0.0294411952966  |              {'C': 10000.0, 'gamma': 10.0}               |        7        |
|  0.582618968089 | 0.0208034022224  |              {'C': 10000.0, 'gamma': 100.0}              |        3        |
|  0.566666881824 | 0.0239607327251  |             {'C': 10000.0, 'gamma': 1000.0}              |        10       |
|  0.548604901655 | 0.0169857721016  |             {'C': 10000.0, 'gamma': 10000.0}             |        19       |
+-----------------+------------------+----------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'C': 1000.0, 'gamma': 100.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      0.92      0.78        74
          3       0.00      0.00      0.00         1
         34       0.25      0.06      0.10        34

avg / total       0.54      0.64      0.56       109


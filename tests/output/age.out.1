Evaluating DummyClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 1 candidates, totalling 10 fits
Grid scores on validation set:

+-----------------+-----------------+--------+-----------------+
| test_mean_score |  test_std_score | params | test_rank_score |
+-----------------+-----------------+--------+-----------------+
|  0.530294696079 | 0.0165304982677 |   {}   |        1        |
+-----------------+-----------------+--------+-----------------+
Best parameters set found on validation set:

{}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.70      1.00      0.83        97
          1       0.00      0.00      0.00        34
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         3

avg / total       0.49      0.70      0.58       138


Average accuracy on test set (using best parameters): 0.70

===================================================================
[ 0.70289855  0.          0.          0.        ]
===================================================================
Evaluating KNeighborsClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 5 candidates, totalling 50 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.562970615673 | 0.0372518691368 |  {'n_neighbors': 3} |        1        |
|  0.54098047076  | 0.0439514783941 |  {'n_neighbors': 5} |        2        |
|  0.537961722686 | 0.0399852564348 | {'n_neighbors': 11} |        3        |
|  0.520043002098 |  0.021556891055 | {'n_neighbors': 21} |        5        |
|  0.527218315745 | 0.0197420070312 | {'n_neighbors': 31} |        4        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.71      0.84      0.77        98
          1       0.22      0.14      0.17        35
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         1

avg / total       0.56      0.63      0.59       138


Average accuracy on test set (using best parameters): 0.63

===================================================================
[ 0.71304348  0.2173913   0.          0.        ]
===================================================================
Evaluating RandomForestClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 7 candidates, totalling 70 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.500421920941 |  0.072583948311 | {'n_estimators': 2}  |        7        |
|  0.509187604016 | 0.0694202200281 | {'n_estimators': 3}  |        4        |
|  0.535874886128 | 0.0796610366235 | {'n_estimators': 5}  |        1        |
|  0.504973524866 | 0.0433364592983 | {'n_estimators': 10} |        6        |
|  0.530643167355 |  0.04557173796  | {'n_estimators': 20} |        2        |
|  0.521441296066 |  0.023153890146 | {'n_estimators': 40} |        3        |
|  0.507111960383 | 0.0242144231892 | {'n_estimators': 60} |        5        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.75      0.79      0.77       104
          1       0.18      0.17      0.18        29
          2       0.00      0.00      0.00         3
          3       0.00      0.00      0.00         2

avg / total       0.60      0.63      0.61       138


Average accuracy on test set (using best parameters): 0.63

===================================================================
[ 0.74545455  0.17857143  0.          0.        ]
===================================================================
Evaluating MLPClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 21 candidates, totalling 210 fits
Grid scores on validation set:

+-----------------+-----------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                         params                        | test_rank_score |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
|  0.530293449904 | 0.0163223606206 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        3        |
|  0.530293449904 | 0.0163223606206 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        3        |
|  0.530293449904 | 0.0163223606206 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        3        |
|  0.530293449904 | 0.0163223606206 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        3        |
|  0.530293449904 | 0.0163223606206 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        3        |
|  0.530293449904 | 0.0163223606206 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        3        |
|  0.530293449904 | 0.0163223606206 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        3        |
|  0.529120692897 | 0.0189944976836 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        11       |
|  0.526700500874 | 0.0166532169755 |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        16       |
|  0.520721980589 | 0.0206680809685 |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        19       |
|  0.527938014984 | 0.0267958917193 |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        14       |
|  0.533506000454 | 0.0247851277721 |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        1        |
|  0.524376247322 | 0.0216193622345 |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        17       |
|  0.528576658835 | 0.0230855957129 |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        13       |
|  0.530293449904 | 0.0163223606206 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        3        |
|  0.527920157254 | 0.0157855277415 |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        15       |
|  0.532559491956 | 0.0180363698442 |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        2        |
|  0.523083196239 | 0.0195645498148 |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        18       |
|  0.520321713874 | 0.0189150826041 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        20       |
|  0.520274783073 | 0.0233758275809 |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        21       |
|  0.52889925303  | 0.0319211985385 |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        12       |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'tanh', 'hidden_layer_sizes': 100}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.70      1.00      0.83        97
          1       0.00      0.00      0.00        33
          2       0.00      0.00      0.00         6
          3       0.00      0.00      0.00         2

avg / total       0.49      0.70      0.58       138


Average accuracy on test set (using best parameters): 0.70

===================================================================
[ 0.70289855  0.          0.          0.        ]
===================================================================
Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 128 candidates, totalling 1280 fits
Grid scores on validation set:

+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                                    params                                    | test_rank_score |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
|  0.545946404581 | 0.0212558706876 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.001}               |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.001}                 |        39       |
|  0.545946404581 | 0.0212558706876 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.01}                |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.01}                 |        39       |
|  0.545946404581 | 0.0212558706876 |        {'kernel': 'linear', 'C': 0.001, 'gamma': 0.10000000000000001}        |        39       |
|  0.545946404581 | 0.0212558706876 |         {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.10000000000000001}          |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'linear', 'C': 0.001, 'gamma': 1.0}                |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 1.0}                  |        39       |
|  0.545946404581 | 0.0212558706876 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 10.0}                |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 10.0}                 |        39       |
|  0.545946404581 | 0.0212558706876 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 100.0}               |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 100.0}                 |        39       |
|  0.545946404581 | 0.0212558706876 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 1000.0}               |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 1000.0}                |        39       |
|  0.545946404581 | 0.0212558706876 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 10000.0}              |        39       |
|  0.545946404581 | 0.0212558706876 |               {'kernel': 'rbf', 'C': 0.001, 'gamma': 10000.0}                |        39       |
|  0.545946404581 | 0.0212558706876 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 0.001}                |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.001}                 |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 0.01}                |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.01}                  |        39       |
|  0.545946404581 | 0.0212558706876 |        {'kernel': 'linear', 'C': 0.01, 'gamma': 0.10000000000000001}         |        39       |
|  0.545946404581 | 0.0212558706876 |          {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.10000000000000001}          |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 1.0}                 |        39       |
|  0.545946404581 | 0.0212558706876 |                  {'kernel': 'rbf', 'C': 0.01, 'gamma': 1.0}                  |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 10.0}                |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 10.0}                  |        39       |
|  0.545946404581 | 0.0212558706876 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 100.0}                |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 100.0}                 |        39       |
|  0.545946404581 | 0.0212558706876 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 1000.0}               |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 1000.0}                 |        39       |
|  0.545946404581 | 0.0212558706876 |              {'kernel': 'linear', 'C': 0.01, 'gamma': 10000.0}               |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 10000.0}                |        39       |
|  0.545946404581 | 0.0212558706876 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.001}        |        39       |
|  0.545946404581 | 0.0212558706876 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.001}          |        39       |
|  0.545946404581 | 0.0212558706876 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.01}         |        39       |
|  0.545946404581 | 0.0212558706876 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.01}          |        39       |
|  0.545946404581 | 0.0212558706876 | {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        39       |
|  0.545946404581 | 0.0212558706876 |  {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001}   |        39       |
|  0.545946404581 | 0.0212558706876 |         {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1.0}         |        39       |
|  0.545946404581 | 0.0212558706876 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1.0}           |        39       |
|  0.545946404581 | 0.0212558706876 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10.0}         |        39       |
|  0.545946404581 | 0.0212558706876 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10.0}          |        39       |
|  0.545946404581 | 0.0212558706876 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 100.0}        |        39       |
|  0.545946404581 | 0.0212558706876 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 100.0}          |        39       |
|  0.545946404581 | 0.0212558706876 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1000.0}        |        39       |
|  0.545946404581 | 0.0212558706876 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1000.0}         |        39       |
|  0.545946404581 | 0.0212558706876 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10000.0}       |        39       |
|  0.545946404581 | 0.0212558706876 |        {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10000.0}         |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.001}                |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.001}                  |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.01}                 |        39       |
|  0.545946404581 | 0.0212558706876 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.01}                  |        39       |
|  0.545946404581 | 0.0212558706876 |         {'kernel': 'linear', 'C': 1.0, 'gamma': 0.10000000000000001}         |        39       |
|  0.545946404581 | 0.0212558706876 |          {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.10000000000000001}           |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'linear', 'C': 1.0, 'gamma': 1.0}                 |        39       |
|  0.545946404581 | 0.0212558706876 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 1.0}                   |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 10.0}                 |        39       |
|  0.545946404581 | 0.0212558706876 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 10.0}                  |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 100.0}                |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 100.0}                  |        39       |
|  0.545946404581 | 0.0212558706876 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 1000.0}                |        39       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 1000.0}                 |        39       |
|  0.545946404581 | 0.0212558706876 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 10000.0}               |        39       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'rbf', 'C': 1.0, 'gamma': 10000.0}                 |        39       |
|  0.551244445648 | 0.0321805995727 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 0.001}                |        27       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.001}                 |        39       |
|  0.551244445648 | 0.0321805995727 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 0.01}                |        27       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.01}                  |        39       |
|  0.551244445648 | 0.0321805995727 |        {'kernel': 'linear', 'C': 10.0, 'gamma': 0.10000000000000001}         |        27       |
|  0.545946404581 | 0.0212558706876 |          {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.10000000000000001}          |        39       |
|  0.551244445648 | 0.0321805995727 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 1.0}                 |        27       |
|  0.548514819545 | 0.0293719135524 |                  {'kernel': 'rbf', 'C': 10.0, 'gamma': 1.0}                  |        35       |
|  0.551244445648 | 0.0321805995727 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 10.0}                |        27       |
|  0.553354770796 | 0.0503153716944 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 10.0}                  |        24       |
|  0.551244445648 | 0.0321805995727 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 100.0}                |        27       |
|  0.582299649895 | 0.0507844706943 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 100.0}                 |        4        |
|  0.551244445648 | 0.0321805995727 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 1000.0}               |        27       |
|  0.566836301164 | 0.0322642284367 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 1000.0}                 |        10       |
|  0.551244445648 | 0.0321805995727 |              {'kernel': 'linear', 'C': 10.0, 'gamma': 10000.0}               |        27       |
|  0.544761595526 | 0.0222268608857 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 10000.0}                |       109       |
|  0.543230363609 | 0.0253513890658 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.001}               |       110       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.001}                 |        39       |
|  0.543230363609 | 0.0253513890658 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.01}                |       110       |
|  0.545946404581 | 0.0212558706876 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.01}                 |        39       |
|  0.543230363609 | 0.0253513890658 |        {'kernel': 'linear', 'C': 100.0, 'gamma': 0.10000000000000001}        |       110       |
|  0.548166854805 | 0.0251614060039 |         {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.10000000000000001}          |        36       |
|  0.543230363609 | 0.0253513890658 |                {'kernel': 'linear', 'C': 100.0, 'gamma': 1.0}                |       110       |
|  0.554058258181 | 0.0276893392078 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 1.0}                  |        23       |
|  0.543230363609 | 0.0253513890658 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 10.0}                |       110       |
|  0.580189698465 | 0.0693649529158 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}                 |        5        |
|  0.543230363609 | 0.0253513890658 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 100.0}               |       110       |
|  0.587331990672 | 0.0608932538855 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 100.0}                 |        1        |
|  0.543230363609 | 0.0253513890658 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 1000.0}               |       110       |
|  0.557636932051 | 0.0257784755889 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 1000.0}                |        14       |
|  0.543230363609 | 0.0253513890658 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 10000.0}              |       110       |
|  0.559449978259 | 0.0446223228823 |               {'kernel': 'rbf', 'C': 100.0, 'gamma': 10000.0}                |        11       |
|  0.554417596626 | 0.0368457741994 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.001}               |        15       |
|  0.545946404581 | 0.0212558706876 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.001}                |        39       |
|  0.554417596626 | 0.0368457741994 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.01}               |        15       |
|  0.548166854805 | 0.0251614060039 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.01}                 |        36       |
|  0.554417596626 | 0.0368457741994 |       {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.10000000000000001}        |        15       |
|  0.55230327534  | 0.0346176008459 |         {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.10000000000000001}         |        25       |
|  0.554417596626 | 0.0368457741994 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 1.0}                |        15       |
|  0.541491221049 |  0.066290019786 |                 {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1.0}                 |       118       |
|  0.554417596626 | 0.0368457741994 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 10.0}               |        15       |
|  0.582940688118 | 0.0519418640145 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10.0}                 |        3        |
|  0.554417596626 | 0.0368457741994 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 100.0}               |        15       |
|  0.569533365282 | 0.0383487800732 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 100.0}                |        6        |
|  0.554417596626 | 0.0368457741994 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 1000.0}              |        15       |
|  0.569010792745 |  0.03615148156  |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}                |        7        |
|  0.554417596626 | 0.0368457741994 |             {'kernel': 'linear', 'C': 1000.0, 'gamma': 10000.0}              |        15       |
|  0.559449978259 | 0.0446223228823 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10000.0}               |        11       |
|  0.521874061492 | 0.0310846958737 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.001}              |       121       |
|  0.548166854805 | 0.0251614060039 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.001}                |        36       |
|  0.521874061492 | 0.0310846958737 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.01}               |       121       |
|  0.552243605647 | 0.0290893328738 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.01}                |        26       |
|  0.521874061492 | 0.0310846958737 |       {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.10000000000000001}       |       121       |
|  0.53866601237  | 0.0488624368688 |        {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.10000000000000001}         |       119       |
|  0.521874061492 | 0.0310846958737 |               {'kernel': 'linear', 'C': 10000.0, 'gamma': 1.0}               |       121       |
|  0.534438590405 | 0.0866587490647 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1.0}                 |       120       |
|  0.521874061492 | 0.0310846958737 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 10.0}               |       121       |
|  0.567206410994 |  0.072703904392 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10.0}                |        9        |
|  0.521874061492 | 0.0310846958737 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 100.0}              |       121       |
|  0.585697794805 | 0.0607246073286 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 100.0}                |        2        |
|  0.521874061492 | 0.0310846958737 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 1000.0}              |       121       |
|  0.569010792745 |  0.03615148156  |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1000.0}               |        7        |
|  0.521874061492 | 0.0310846958737 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 10000.0}             |       121       |
|  0.559449978259 | 0.0446223228823 |              {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10000.0}               |        11       |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'kernel': 'rbf', 'C': 100.0, 'gamma': 100.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.65      0.89      0.75        92
          1       0.09      0.02      0.04        43
          2       0.00      0.00      0.00         2
          3       0.00      0.00      0.00         1

avg / total       0.46      0.60      0.51       138


Average accuracy on test set (using best parameters): 0.60

===================================================================
[ 0.64566929  0.09090909  0.          0.        ]
===================================================================

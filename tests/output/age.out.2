Evaluating DummyClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 1 candidates, totalling 10 fits
Grid scores on validation set:

+-----------------+-----------------+--------+-----------------+
| test_mean_score |  test_std_score | params | test_rank_score |
+-----------------+-----------------+--------+-----------------+
|  0.558518134422 | 0.0209717083188 |   {}   |        1        |
+-----------------+-----------------+--------+-----------------+
Best parameters set found on validation set:

{}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.64      1.00      0.78        88
          1       0.00      0.00      0.00        38
          2       0.00      0.00      0.00         9
          3       0.00      0.00      0.00         3

avg / total       0.41      0.64      0.50       138


Average accuracy on test set (using best parameters): 0.64

===================================================================
[ 0.63768116  0.          0.          0.        ]
===================================================================
Evaluating KNeighborsClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 5 candidates, totalling 50 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.553079004116 | 0.0475827140777 |  {'n_neighbors': 3} |        3        |
|  0.567635296183 | 0.0409865558827 |  {'n_neighbors': 5} |        1        |
|  0.566272950183 | 0.0438918344487 | {'n_neighbors': 11} |        2        |
|  0.545746715741 | 0.0320226519973 | {'n_neighbors': 21} |        4        |
|  0.542846008784 | 0.0238431185988 | {'n_neighbors': 31} |        5        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      0.92      0.78        93
          1       0.30      0.08      0.12        39
          2       0.00      0.00      0.00         5
          3       0.00      0.00      0.00         1

avg / total       0.54      0.64      0.56       138


Average accuracy on test set (using best parameters): 0.64

===================================================================
[ 0.671875  0.3       0.        0.      ]
===================================================================
Evaluating RandomForestClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 7 candidates, totalling 70 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.551614328408 | 0.0441957800646 | {'n_estimators': 2}  |        5        |
|  0.522813007468 | 0.0387555179977 | {'n_estimators': 3}  |        7        |
|  0.567818525421 | 0.0903354338388 | {'n_estimators': 5}  |        1        |
|  0.538753843126 | 0.0388807041169 | {'n_estimators': 10} |        6        |
|  0.560443667279 | 0.0482331144899 | {'n_estimators': 20} |        2        |
|  0.552454241785 | 0.0484952115109 | {'n_estimators': 40} |        4        |
|  0.554282743564 | 0.0476320287008 | {'n_estimators': 60} |        3        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.65      0.80      0.72        94
          1       0.17      0.10      0.13        41
          2       0.00      0.00      0.00         3

avg / total       0.50      0.57      0.53       138


Average accuracy on test set (using best parameters): 0.57

===================================================================
[ 0.65217391  0.17391304  0.        ]
===================================================================
Evaluating MLPClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 21 candidates, totalling 210 fits
Grid scores on validation set:

+-----------------+-----------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                         params                        | test_rank_score |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
|  0.54915903167  |  0.027274705725 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        14       |
|  0.54915903167  |  0.027274705725 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        14       |
|  0.54915903167  |  0.027274705725 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        14       |
|  0.54915903167  |  0.027274705725 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        14       |
|  0.54915903167  |  0.027274705725 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        14       |
|  0.54915903167  |  0.027274705725 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        14       |
|  0.54915903167  |  0.027274705725 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        14       |
|  0.554457072737 | 0.0359673082403 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        10       |
|  0.560819311769 | 0.0426041403521 |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        5        |
|  0.566465918423 |  0.049058818134 |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        2        |
|  0.556686873573 | 0.0457124487723 |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        8        |
|  0.553871631897 | 0.0436649912221 |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        12       |
|  0.559447586497 | 0.0606592653511 |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        7        |
|  0.569673827555 |  0.063751235976 |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        1        |
|  0.54796278021  | 0.0263028378956 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        21       |
|  0.554457072737 | 0.0359673082403 |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        10       |
|  0.552081972025 | 0.0368737032929 |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        13       |
|  0.555698418467 | 0.0436427121974 |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        9        |
|  0.560996459534 | 0.0488378146318 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        4        |
|  0.562529233329 | 0.0502424154875 |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        3        |
|  0.560053608882 |  0.054930076277 |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        6        |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'tanh', 'hidden_layer_sizes': 150}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.66      1.00      0.79        91
          1       0.00      0.00      0.00        39
          2       0.00      0.00      0.00         5
          3       0.00      0.00      0.00         3

avg / total       0.43      0.66      0.52       138


Average accuracy on test set (using best parameters): 0.66

===================================================================
[ 0.65942029  0.          0.          0.        ]
===================================================================
Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 128 candidates, totalling 1280 fits
Grid scores on validation set:

+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                                    params                                    | test_rank_score |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
|  0.533361645019 | 0.0103342456036 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.001}               |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.001}                 |        58       |
|  0.533361645019 | 0.0103342456036 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.01}                |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.01}                 |        58       |
|  0.533361645019 | 0.0103342456036 |        {'kernel': 'linear', 'C': 0.001, 'gamma': 0.10000000000000001}        |        58       |
|  0.533361645019 | 0.0103342456036 |         {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.10000000000000001}          |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'linear', 'C': 0.001, 'gamma': 1.0}                |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 1.0}                  |        58       |
|  0.533361645019 | 0.0103342456036 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 10.0}                |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 10.0}                 |        58       |
|  0.533361645019 | 0.0103342456036 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 100.0}               |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 100.0}                 |        58       |
|  0.533361645019 | 0.0103342456036 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 1000.0}               |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 1000.0}                |        58       |
|  0.533361645019 | 0.0103342456036 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 10000.0}              |        58       |
|  0.533361645019 | 0.0103342456036 |               {'kernel': 'rbf', 'C': 0.001, 'gamma': 10000.0}                |        58       |
|  0.533361645019 | 0.0103342456036 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 0.001}                |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.001}                 |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 0.01}                |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.01}                  |        58       |
|  0.533361645019 | 0.0103342456036 |        {'kernel': 'linear', 'C': 0.01, 'gamma': 0.10000000000000001}         |        58       |
|  0.533361645019 | 0.0103342456036 |          {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.10000000000000001}          |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 1.0}                 |        58       |
|  0.533361645019 | 0.0103342456036 |                  {'kernel': 'rbf', 'C': 0.01, 'gamma': 1.0}                  |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 10.0}                |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 10.0}                  |        58       |
|  0.533361645019 | 0.0103342456036 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 100.0}                |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 100.0}                 |        58       |
|  0.533361645019 | 0.0103342456036 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 1000.0}               |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 1000.0}                 |        58       |
|  0.533361645019 | 0.0103342456036 |              {'kernel': 'linear', 'C': 0.01, 'gamma': 10000.0}               |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 10000.0}                |        58       |
|  0.533361645019 | 0.0103342456036 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.001}        |        58       |
|  0.533361645019 | 0.0103342456036 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.001}          |        58       |
|  0.533361645019 | 0.0103342456036 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.01}         |        58       |
|  0.533361645019 | 0.0103342456036 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.01}          |        58       |
|  0.533361645019 | 0.0103342456036 | {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        58       |
|  0.533361645019 | 0.0103342456036 |  {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001}   |        58       |
|  0.533361645019 | 0.0103342456036 |         {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1.0}         |        58       |
|  0.533361645019 | 0.0103342456036 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1.0}           |        58       |
|  0.533361645019 | 0.0103342456036 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10.0}         |        58       |
|  0.533361645019 | 0.0103342456036 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10.0}          |        58       |
|  0.533361645019 | 0.0103342456036 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 100.0}        |        58       |
|  0.533361645019 | 0.0103342456036 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 100.0}          |        58       |
|  0.533361645019 | 0.0103342456036 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1000.0}        |        58       |
|  0.533361645019 | 0.0103342456036 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1000.0}         |        58       |
|  0.533361645019 | 0.0103342456036 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10000.0}       |        58       |
|  0.533361645019 | 0.0103342456036 |        {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10000.0}         |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.001}                |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.001}                  |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.01}                 |        58       |
|  0.533361645019 | 0.0103342456036 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.01}                  |        58       |
|  0.533361645019 | 0.0103342456036 |         {'kernel': 'linear', 'C': 1.0, 'gamma': 0.10000000000000001}         |        58       |
|  0.533361645019 | 0.0103342456036 |          {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.10000000000000001}           |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'linear', 'C': 1.0, 'gamma': 1.0}                 |        58       |
|  0.533361645019 | 0.0103342456036 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 1.0}                   |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 10.0}                 |        58       |
|  0.533361645019 | 0.0103342456036 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 10.0}                  |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 100.0}                |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 100.0}                  |        58       |
|  0.533361645019 | 0.0103342456036 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 1000.0}                |        58       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 1000.0}                 |        58       |
|  0.533361645019 | 0.0103342456036 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 10000.0}               |        58       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'rbf', 'C': 1.0, 'gamma': 10000.0}                 |        58       |
|  0.537458567174 | 0.0194037568749 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 0.001}                |        38       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.001}                 |        58       |
|  0.537458567174 | 0.0194037568749 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 0.01}                |        38       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.01}                  |        58       |
|  0.537458567174 | 0.0194037568749 |        {'kernel': 'linear', 'C': 10.0, 'gamma': 0.10000000000000001}         |        38       |
|  0.533361645019 | 0.0103342456036 |          {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.10000000000000001}          |        58       |
|  0.537458567174 | 0.0194037568749 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 1.0}                 |        38       |
|  0.536278245436 | 0.0135643272139 |                  {'kernel': 'rbf', 'C': 10.0, 'gamma': 1.0}                  |        56       |
|  0.537458567174 | 0.0194037568749 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 10.0}                |        38       |
|  0.574622883903 | 0.0455487866347 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 10.0}                  |        5        |
|  0.537458567174 | 0.0194037568749 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 100.0}                |        38       |
|  0.545795495486 | 0.0361826186152 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 100.0}                 |        31       |
|  0.537458567174 | 0.0194037568749 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 1000.0}               |        38       |
|  0.531799138647 | 0.0143292667671 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 1000.0}                 |       128       |
|  0.537458567174 | 0.0194037568749 |              {'kernel': 'linear', 'C': 10.0, 'gamma': 10000.0}               |        38       |
|  0.537443651463 | 0.0157498157821 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 10000.0}                |        46       |
|  0.553715612353 | 0.0333618031275 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.001}               |        20       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.001}                 |        58       |
|  0.553715612353 | 0.0333618031275 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.01}                |        20       |
|  0.533361645019 | 0.0103342456036 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.01}                 |        58       |
|  0.553715612353 | 0.0333618031275 |        {'kernel': 'linear', 'C': 100.0, 'gamma': 0.10000000000000001}        |        20       |
|  0.542752652106 | 0.0265713601872 |         {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.10000000000000001}          |        34       |
|  0.553715612353 | 0.0333618031275 |                {'kernel': 'linear', 'C': 100.0, 'gamma': 1.0}                |        20       |
|  0.54399427024  | 0.0516929945408 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 1.0}                  |        33       |
|  0.553715612353 | 0.0333618031275 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 10.0}                |        20       |
|  0.580921958117 | 0.0315949926874 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}                 |        1        |
|  0.553715612353 | 0.0333618031275 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 100.0}               |        20       |
|  0.555907049531 | 0.0564013766754 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 100.0}                 |        17       |
|  0.553715612353 | 0.0333618031275 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 1000.0}               |        20       |
|  0.53754879718  | 0.0217572864434 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 1000.0}                |        37       |
|  0.553715612353 | 0.0333618031275 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 10000.0}              |        20       |
|  0.551471490035 | 0.0382488616136 |               {'kernel': 'rbf', 'C': 100.0, 'gamma': 10000.0}                |        28       |
|  0.536850923105 | 0.0469727256309 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.001}               |        48       |
|  0.533361645019 | 0.0103342456036 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.001}                |        58       |
|  0.536850923105 | 0.0469727256309 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.01}               |        48       |
|  0.542752652106 | 0.0265713601872 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.01}                 |        34       |
|  0.536850923105 | 0.0469727256309 |       {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.10000000000000001}        |        48       |
|  0.544964657881 | 0.0365399677553 |         {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.10000000000000001}         |        32       |
|  0.536850923105 | 0.0469727256309 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 1.0}                |        48       |
|  0.537303721483 | 0.0455445201418 |                 {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1.0}                 |        47       |
|  0.536850923105 | 0.0469727256309 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 10.0}               |        48       |
|  0.574912525538 | 0.0497782549371 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10.0}                 |        4        |
|  0.536850923105 | 0.0469727256309 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 100.0}               |        48       |
|  0.577193336268 | 0.0699208772621 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 100.0}                |        2        |
|  0.536850923105 | 0.0469727256309 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 1000.0}              |        48       |
|  0.554807368014 | 0.0344627830596 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}                |        18       |
|  0.536850923105 | 0.0469727256309 |             {'kernel': 'linear', 'C': 1000.0, 'gamma': 10000.0}              |        48       |
|  0.551471490035 | 0.0382488616136 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10000.0}               |        28       |
|  0.569059683481 |  0.052239842328 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.001}              |        7        |
|  0.542752652106 | 0.0265713601872 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.001}                |        34       |
|  0.569059683481 |  0.052239842328 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.01}               |        7        |
|  0.560413908677 | 0.0302441961673 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.01}                |        16       |
|  0.569059683481 |  0.052239842328 |       {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.10000000000000001}       |        7        |
|  0.563488102205 | 0.0748757514467 |        {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.10000000000000001}         |        15       |
|  0.569059683481 |  0.052239842328 |               {'kernel': 'linear', 'C': 10000.0, 'gamma': 1.0}               |        7        |
|  0.534681347451 | 0.0442878502875 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1.0}                 |        57       |
|  0.569059683481 |  0.052239842328 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 10.0}               |        7        |
|  0.571002445243 | 0.0481846477367 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10.0}                |        6        |
|  0.569059683481 |  0.052239842328 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 100.0}              |        7        |
|  0.575806033174 | 0.0738072711307 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 100.0}                |        3        |
|  0.569059683481 |  0.052239842328 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 1000.0}              |        7        |
|  0.554807368014 | 0.0344627830596 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1000.0}               |        18       |
|  0.569059683481 |  0.052239842328 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 10000.0}             |        7        |
|  0.551471490035 | 0.0382488616136 |              {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10000.0}               |        28       |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.74      0.84      0.79        96
          1       0.29      0.24      0.26        33
          2       0.00      0.00      0.00         8
          3       0.00      0.00      0.00         1

avg / total       0.58      0.64      0.61       138


Average accuracy on test set (using best parameters): 0.64

===================================================================
[ 0.73636364  0.28571429  0.          0.        ]
===================================================================

Evaluating DummyClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 1 candidates, totalling 10 fits
Grid scores on validation set:

+-----------------+-----------------+--------+-----------------+
| test_mean_score |  test_std_score | params | test_rank_score |
+-----------------+-----------------+--------+-----------------+
|  0.542846008784 | 0.0238431185988 |   {}   |        1        |
+-----------------+-----------------+--------+-----------------+
Best parameters set found on validation set:

{}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      1.00      0.81        93
          1       0.00      0.00      0.00        41
          2       0.00      0.00      0.00         2
          3       0.00      0.00      0.00         2

avg / total       0.45      0.67      0.54       138


Average accuracy on test set (using best parameters): 0.67

===================================================================
[ 0.67391304  0.          0.          0.        ]
===================================================================
Evaluating KNeighborsClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 5 candidates, totalling 50 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.528202257808 |  0.057052525829 |  {'n_neighbors': 3} |        5        |
|  0.550452980494 | 0.0436338238812 |  {'n_neighbors': 5} |        1        |
|  0.534211336587 | 0.0367011099232 | {'n_neighbors': 11} |        3        |
|  0.533501667455 |  0.022893670407 | {'n_neighbors': 21} |        4        |
|  0.539676405671 | 0.0199909647354 | {'n_neighbors': 31} |        2        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.70      0.91      0.80        94
          1       0.44      0.19      0.27        36
          2       0.00      0.00      0.00         5
          3       0.00      0.00      0.00         3

avg / total       0.59      0.67      0.61       138


Average accuracy on test set (using best parameters): 0.67

===================================================================
[ 0.70491803  0.4375      0.          0.        ]
===================================================================
Evaluating RandomForestClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 7 candidates, totalling 70 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.538236550384 | 0.0639843318502 | {'n_estimators': 2}  |        3        |
|  0.521035715976 | 0.0626069903708 | {'n_estimators': 3}  |        7        |
|  0.552487786572 | 0.0428710891968 | {'n_estimators': 5}  |        2        |
|  0.522895633875 | 0.0594669635202 | {'n_estimators': 10} |        6        |
|  0.529309927382 | 0.0432275656282 | {'n_estimators': 20} |        5        |
|  0.559522339491 | 0.0524495343547 | {'n_estimators': 40} |        1        |
|  0.530702372249 | 0.0368028517751 | {'n_estimators': 60} |        4        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 40}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.71      0.90      0.79        99
          1       0.23      0.09      0.13        35
          2       0.00      0.00      0.00         3
          3       0.00      0.00      0.00         1

avg / total       0.57      0.67      0.60       138


Average accuracy on test set (using best parameters): 0.67

===================================================================
[ 0.712       0.23076923  0.          0.        ]
===================================================================
Evaluating MLPClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 21 candidates, totalling 210 fits
Grid scores on validation set:

+-----------------+-----------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                         params                        | test_rank_score |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
|  0.524117314652 | 0.0196214251051 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        13       |
|  0.524117314652 | 0.0196214251051 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        13       |
|  0.524117314652 | 0.0196214251051 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        13       |
|  0.524117314652 | 0.0196214251051 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        13       |
|  0.524117314652 | 0.0196214251051 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        13       |
|  0.524117314652 | 0.0196214251051 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        13       |
|  0.524117314652 | 0.0196214251051 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        13       |
|  0.528203489319 | 0.0196490311703 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        9        |
|  0.527015005725 | 0.0190149036031 |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        11       |
|  0.527226059074 | 0.0209579095162 |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        10       |
|  0.534608170457 |  0.020491588495 |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        6        |
|  0.536299841419 | 0.0291298970652 |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        3        |
|  0.540480369658 | 0.0384055870037 |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        2        |
|  0.559984813525 | 0.0425355846401 |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        1        |
|  0.524117314652 | 0.0196214251051 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        13       |
|  0.521703778738 | 0.0185178130301 |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        21       |
|  0.524613856181 | 0.0207178241001 |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        12       |
|  0.529336663802 | 0.0198403633625 |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        8        |
|  0.534107561412 | 0.0152535797161 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        7        |
|  0.535074425447 | 0.0247192522542 |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        5        |
|  0.53542559337  | 0.0260907151754 |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        4        |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'tanh', 'hidden_layer_sizes': 150}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.72      1.00      0.84        99
          1       0.00      0.00      0.00        29
          2       0.00      0.00      0.00         8
          3       0.00      0.00      0.00         2

avg / total       0.51      0.72      0.60       138


Average accuracy on test set (using best parameters): 0.72

===================================================================
[ 0.7173913  0.         0.         0.       ]
===================================================================
Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 128 candidates, totalling 1280 fits
Grid scores on validation set:

+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                                    params                                    | test_rank_score |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
|  0.558528341066 | 0.0217299977606 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.001}               |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.001}                 |        24       |
|  0.558528341066 | 0.0217299977606 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.01}                |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.01}                 |        24       |
|  0.558528341066 | 0.0217299977606 |        {'kernel': 'linear', 'C': 0.001, 'gamma': 0.10000000000000001}        |        24       |
|  0.558528341066 | 0.0217299977606 |         {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.10000000000000001}          |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'linear', 'C': 0.001, 'gamma': 1.0}                |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 1.0}                  |        24       |
|  0.558528341066 | 0.0217299977606 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 10.0}                |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 10.0}                 |        24       |
|  0.558528341066 | 0.0217299977606 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 100.0}               |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 100.0}                 |        24       |
|  0.558528341066 | 0.0217299977606 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 1000.0}               |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 1000.0}                |        24       |
|  0.558528341066 | 0.0217299977606 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 10000.0}              |        24       |
|  0.558528341066 | 0.0217299977606 |               {'kernel': 'rbf', 'C': 0.001, 'gamma': 10000.0}                |        24       |
|  0.558528341066 | 0.0217299977606 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 0.001}                |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.001}                 |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 0.01}                |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.01}                  |        24       |
|  0.558528341066 | 0.0217299977606 |        {'kernel': 'linear', 'C': 0.01, 'gamma': 0.10000000000000001}         |        24       |
|  0.558528341066 | 0.0217299977606 |          {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.10000000000000001}          |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 1.0}                 |        24       |
|  0.558528341066 | 0.0217299977606 |                  {'kernel': 'rbf', 'C': 0.01, 'gamma': 1.0}                  |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 10.0}                |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 10.0}                  |        24       |
|  0.558528341066 | 0.0217299977606 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 100.0}                |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 100.0}                 |        24       |
|  0.558528341066 | 0.0217299977606 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 1000.0}               |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 1000.0}                 |        24       |
|  0.558528341066 | 0.0217299977606 |              {'kernel': 'linear', 'C': 0.01, 'gamma': 10000.0}               |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 10000.0}                |        24       |
|  0.558528341066 | 0.0217299977606 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.001}        |        24       |
|  0.558528341066 | 0.0217299977606 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.001}          |        24       |
|  0.558528341066 | 0.0217299977606 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.01}         |        24       |
|  0.558528341066 | 0.0217299977606 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.01}          |        24       |
|  0.558528341066 | 0.0217299977606 | {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        24       |
|  0.558528341066 | 0.0217299977606 |  {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001}   |        24       |
|  0.558528341066 | 0.0217299977606 |         {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1.0}         |        24       |
|  0.558528341066 | 0.0217299977606 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1.0}           |        24       |
|  0.558528341066 | 0.0217299977606 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10.0}         |        24       |
|  0.558528341066 | 0.0217299977606 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10.0}          |        24       |
|  0.558528341066 | 0.0217299977606 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 100.0}        |        24       |
|  0.558528341066 | 0.0217299977606 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 100.0}          |        24       |
|  0.558528341066 | 0.0217299977606 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1000.0}        |        24       |
|  0.558528341066 | 0.0217299977606 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1000.0}         |        24       |
|  0.558528341066 | 0.0217299977606 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10000.0}       |        24       |
|  0.558528341066 | 0.0217299977606 |        {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10000.0}         |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.001}                |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.001}                  |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.01}                 |        24       |
|  0.558528341066 | 0.0217299977606 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.01}                  |        24       |
|  0.558528341066 | 0.0217299977606 |         {'kernel': 'linear', 'C': 1.0, 'gamma': 0.10000000000000001}         |        24       |
|  0.558528341066 | 0.0217299977606 |          {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.10000000000000001}           |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'linear', 'C': 1.0, 'gamma': 1.0}                 |        24       |
|  0.558528341066 | 0.0217299977606 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 1.0}                   |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 10.0}                 |        24       |
|  0.558528341066 | 0.0217299977606 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 10.0}                  |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 100.0}                |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 100.0}                  |        24       |
|  0.558528341066 | 0.0217299977606 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 1000.0}                |        24       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 1000.0}                 |        24       |
|  0.558528341066 | 0.0217299977606 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 10000.0}               |        24       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'rbf', 'C': 1.0, 'gamma': 10000.0}                 |        24       |
|  0.557332089605 | 0.0210366017158 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 0.001}                |        94       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.001}                 |        24       |
|  0.557332089605 | 0.0210366017158 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 0.01}                |        94       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.01}                  |        24       |
|  0.557332089605 | 0.0210366017158 |        {'kernel': 'linear', 'C': 10.0, 'gamma': 0.10000000000000001}         |        94       |
|  0.558528341066 | 0.0217299977606 |          {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.10000000000000001}          |        24       |
|  0.557332089605 | 0.0210366017158 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 1.0}                 |        94       |
|  0.560232004154 | 0.0221575630732 |                  {'kernel': 'rbf', 'C': 10.0, 'gamma': 1.0}                  |        15       |
|  0.557332089605 | 0.0210366017158 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 10.0}                |        94       |
|  0.547093206743 | 0.0492760811826 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 10.0}                  |       107       |
|  0.557332089605 | 0.0210366017158 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 100.0}                |        94       |
|  0.561228015582 | 0.0368720373081 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 100.0}                 |        14       |
|  0.557332089605 | 0.0210366017158 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 1000.0}               |        94       |
|  0.565556326979 | 0.0384023316646 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 1000.0}                 |        8        |
|  0.557332089605 | 0.0210366017158 |              {'kernel': 'linear', 'C': 10.0, 'gamma': 10000.0}               |        94       |
|  0.557332089605 | 0.0210366017158 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 10000.0}                |        94       |
|  0.55886679934  | 0.0297658044032 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.001}               |        16       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.001}                 |        24       |
|  0.55886679934  | 0.0297658044032 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.01}                |        16       |
|  0.558528341066 | 0.0217299977606 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.01}                 |        24       |
|  0.55886679934  | 0.0297658044032 |        {'kernel': 'linear', 'C': 100.0, 'gamma': 0.10000000000000001}        |        16       |
|  0.562624507075 |  0.023098221577 |         {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.10000000000000001}          |        11       |
|  0.55886679934  | 0.0297658044032 |                {'kernel': 'linear', 'C': 100.0, 'gamma': 1.0}                |        16       |
|  0.553768917179 | 0.0685539326745 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 1.0}                  |       104       |
|  0.55886679934  | 0.0297658044032 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 10.0}                |        16       |
|  0.564051969797 | 0.0604207108402 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}                 |        10       |
|  0.55886679934  | 0.0297658044032 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 100.0}               |        16       |
|  0.564619453595 | 0.0342814035965 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 100.0}                 |        9        |
|  0.55886679934  | 0.0297658044032 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 1000.0}               |        16       |
|  0.566527473084 | 0.0338815818269 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 1000.0}                |        7        |
|  0.55886679934  | 0.0297658044032 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 10000.0}              |        16       |
|  0.570753898856 | 0.0356970707818 |               {'kernel': 'rbf', 'C': 100.0, 'gamma': 10000.0}                |        4        |
|  0.51814851054  | 0.0596902936355 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.001}               |       121       |
|  0.558528341066 | 0.0217299977606 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.001}                |        24       |
|  0.51814851054  | 0.0596902936355 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.01}               |       121       |
|  0.56143969802  | 0.0248046342422 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.01}                 |        12       |
|  0.51814851054  | 0.0596902936355 |       {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.10000000000000001}        |       121       |
|  0.573444182288 | 0.0506712828656 |         {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.10000000000000001}         |        3        |
|  0.51814851054  | 0.0596902936355 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 1.0}                |       121       |
|  0.542621480882 | 0.0678554370969 |                 {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1.0}                 |       109       |
|  0.51814851054  | 0.0596902936355 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 10.0}               |       121       |
|  0.551862565952 | 0.0714805352936 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10.0}                 |       105       |
|  0.51814851054  | 0.0596902936355 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 100.0}               |       121       |
|  0.54249004795  | 0.0371645262049 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 100.0}                |       110       |
|  0.51814851054  | 0.0596902936355 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 1000.0}              |       121       |
|  0.577944468997 | 0.0404699153163 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}                |        1        |
|  0.51814851054  | 0.0596902936355 |             {'kernel': 'linear', 'C': 1000.0, 'gamma': 10000.0}              |       121       |
|  0.570753898856 | 0.0356970707818 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10000.0}               |        4        |
|  0.540056289438 |  0.06874331841  |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.001}              |       111       |
|  0.56143969802  | 0.0248046342422 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.001}                |        12       |
|  0.540056289438 |  0.06874331841  |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.01}               |       111       |
|  0.554130592226 | 0.0378269549561 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.01}                |       103       |
|  0.540056289438 |  0.06874331841  |       {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.10000000000000001}       |       111       |
|  0.536683378218 | 0.0676286185487 |        {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.10000000000000001}         |       119       |
|  0.540056289438 |  0.06874331841  |               {'kernel': 'linear', 'C': 10000.0, 'gamma': 1.0}               |       111       |
|  0.547408970764 | 0.0637223235158 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1.0}                 |       106       |
|  0.540056289438 |  0.06874331841  |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 10.0}               |       111       |
|  0.531275719291 | 0.0504040823237 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10.0}                |       120       |
|  0.540056289438 |  0.06874331841  |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 100.0}              |       111       |
|  0.544518153082 | 0.0427545455618 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 100.0}                |       108       |
|  0.540056289438 |  0.06874331841  |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 1000.0}              |       111       |
|  0.577944468997 | 0.0404699153163 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1000.0}               |        1        |
|  0.540056289438 |  0.06874331841  |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 10000.0}             |       111       |
|  0.570753898856 | 0.0356970707818 |              {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10000.0}               |        4        |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.64      0.99      0.78        88
          1       0.67      0.05      0.09        40
          2       0.00      0.00      0.00         7
          3       0.00      0.00      0.00         3

avg / total       0.60      0.64      0.52       138


Average accuracy on test set (using best parameters): 0.64

===================================================================
[ 0.64444444  0.66666667  0.          0.        ]
===================================================================

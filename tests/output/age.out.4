Evaluating DummyClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 1 candidates, totalling 10 fits
Grid scores on validation set:

+-----------------+-----------------+--------+-----------------+
| test_mean_score |  test_std_score | params | test_rank_score |
+-----------------+-----------------+--------+-----------------+
|  0.536600586383 | 0.0240624513335 |   {}   |        1        |
+-----------------+-----------------+--------+-----------------+
Best parameters set found on validation set:

{}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.69      1.00      0.82        95
          1       0.00      0.00      0.00        38
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         1

avg / total       0.47      0.69      0.56       138


Average accuracy on test set (using best parameters): 0.69

===================================================================
[ 0.6884058  0.         0.         0.       ]
===================================================================
Evaluating KNeighborsClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 5 candidates, totalling 50 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.564124314417 | 0.0573813797094 |  {'n_neighbors': 3} |        1        |
|  0.562261484433 | 0.0596654961815 |  {'n_neighbors': 5} |        2        |
|  0.551230440282 | 0.0504941849627 | {'n_neighbors': 11} |        3        |
|  0.538106924479 | 0.0298993153662 | {'n_neighbors': 21} |        5        |
|  0.541689409468 |  0.027082058146 | {'n_neighbors': 31} |        4        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.65      0.86      0.74        93
          1       0.14      0.05      0.07        40
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         1

avg / total       0.48      0.59      0.52       138


Average accuracy on test set (using best parameters): 0.59

===================================================================
[ 0.6504065   0.14285714  0.          0.        ]
===================================================================
Evaluating RandomForestClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 7 candidates, totalling 70 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.585862134904 |  0.053410658153 | {'n_estimators': 2}  |        2        |
|  0.58904041138  | 0.0396245951389 | {'n_estimators': 3}  |        1        |
|  0.558956076571 | 0.0598075879708 | {'n_estimators': 5}  |        4        |
|  0.58473420396  | 0.0456566171443 | {'n_estimators': 10} |        3        |
|  0.555911292507 | 0.0383384098691 | {'n_estimators': 20} |        6        |
|  0.557727526441 | 0.0424126420051 | {'n_estimators': 40} |        5        |
|  0.550754306664 | 0.0380087039213 | {'n_estimators': 60} |        7        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.71      0.80      0.76        92
          1       0.41      0.34      0.37        38
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         4

avg / total       0.59      0.63      0.61       138


Average accuracy on test set (using best parameters): 0.63

===================================================================
[ 0.71153846  0.40625     0.          0.        ]
===================================================================
Evaluating MLPClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 21 candidates, totalling 210 fits
Grid scores on validation set:

+-----------------+-----------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                         params                        | test_rank_score |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
|  0.536611595145 | 0.0249016041914 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        14       |
|  0.536611595145 | 0.0249016041914 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        14       |
|  0.536611595145 | 0.0249016041914 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        14       |
|  0.536611595145 | 0.0249016041914 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        14       |
|  0.536611595145 | 0.0249016041914 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        14       |
|  0.536611595145 | 0.0249016041914 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        14       |
|  0.536611595145 | 0.0249016041914 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        14       |
|  0.540680060604 | 0.0301664069547 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        11       |
|  0.544760692627 | 0.0308217270306 |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        4        |
|  0.538306785341 | 0.0305784005681 |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        13       |
|  0.544603688455 | 0.0275847976935 |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        5        |
|  0.542843882372 | 0.0238670761083 |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        8        |
|  0.543109854359 | 0.0235980386358 |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        7        |
|  0.542000867785 |  0.030218654631 |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        9        |
|  0.536611595145 | 0.0249016041914 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        14       |
|  0.541874486616 | 0.0309042397829 |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        10       |
|  0.540680060604 | 0.0301664069547 |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        11       |
|  0.543566266616 | 0.0301962522213 |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        6        |
|  0.548341936878 | 0.0229195639802 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        3        |
|  0.552678676215 | 0.0300427753034 |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        1        |
|  0.55175022219  | 0.0294270614233 |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        2        |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'relu', 'hidden_layer_sizes': 120}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.69      1.00      0.82        95
          1       0.00      0.00      0.00        39
          2       0.00      0.00      0.00         3
          3       0.00      0.00      0.00         1

avg / total       0.47      0.69      0.56       138


Average accuracy on test set (using best parameters): 0.69

===================================================================
[ 0.6884058  0.         0.         0.       ]
===================================================================
Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 128 candidates, totalling 1280 fits
Grid scores on validation set:

+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                                    params                                    | test_rank_score |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.001}               |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.001}                 |        40       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.01}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.01}                 |        40       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.001, 'gamma': 0.10000000000000001}        |        40       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.10000000000000001}          |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 0.001, 'gamma': 1.0}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 1.0}                  |        40       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 10.0}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 10.0}                 |        40       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 100.0}               |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 100.0}                 |        40       |
|  0.549113550185 | 0.0238151223172 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 1000.0}               |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 1000.0}                |        40       |
|  0.549113550185 | 0.0238151223172 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 10000.0}              |        40       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'rbf', 'C': 0.001, 'gamma': 10000.0}                |        40       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 0.001}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.001}                 |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 0.01}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.01}                  |        40       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.01, 'gamma': 0.10000000000000001}         |        40       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.10000000000000001}          |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 1.0}                 |        40       |
|  0.549113550185 | 0.0238151223172 |                  {'kernel': 'rbf', 'C': 0.01, 'gamma': 1.0}                  |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 10.0}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 10.0}                  |        40       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 100.0}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 100.0}                 |        40       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 1000.0}               |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 1000.0}                 |        40       |
|  0.549113550185 | 0.0238151223172 |              {'kernel': 'linear', 'C': 0.01, 'gamma': 10000.0}               |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 10000.0}                |        40       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.001}        |        40       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.001}          |        40       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.01}         |        40       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.01}          |        40       |
|  0.549113550185 | 0.0238151223172 | {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        40       |
|  0.549113550185 | 0.0238151223172 |  {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001}   |        40       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1.0}         |        40       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1.0}           |        40       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10.0}         |        40       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10.0}          |        40       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 100.0}        |        40       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 100.0}          |        40       |
|  0.549113550185 | 0.0238151223172 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1000.0}        |        40       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1000.0}         |        40       |
|  0.549113550185 | 0.0238151223172 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10000.0}       |        40       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10000.0}         |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.001}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.001}                  |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.01}                 |        40       |
|  0.549113550185 | 0.0238151223172 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.01}                  |        40       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'linear', 'C': 1.0, 'gamma': 0.10000000000000001}         |        40       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.10000000000000001}           |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'linear', 'C': 1.0, 'gamma': 1.0}                 |        40       |
|  0.549113550185 | 0.0238151223172 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 1.0}                   |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 10.0}                 |        40       |
|  0.549113550185 | 0.0238151223172 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 10.0}                  |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 100.0}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 100.0}                  |        40       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 1000.0}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 1000.0}                 |        40       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 10000.0}               |        40       |
|  0.547917298724 | 0.0226931916084 |                {'kernel': 'rbf', 'C': 1.0, 'gamma': 10000.0}                 |       118       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 0.001}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.001}                 |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 0.01}                |        40       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.01}                  |        40       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 10.0, 'gamma': 0.10000000000000001}         |        40       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.10000000000000001}          |        40       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 1.0}                 |        40       |
|  0.550841215778 | 0.0337548088458 |                  {'kernel': 'rbf', 'C': 10.0, 'gamma': 1.0}                  |        39       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 10.0}                |        40       |
|  0.556939616394 | 0.0395516064998 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 10.0}                  |        25       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 100.0}                |        40       |
|  0.53966816167  | 0.0348960879825 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 100.0}                 |       124       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 1000.0}               |        40       |
|  0.556498783911 | 0.0200646400955 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 1000.0}                 |        26       |
|  0.549113550185 | 0.0238151223172 |              {'kernel': 'linear', 'C': 10.0, 'gamma': 10000.0}               |        40       |
|  0.562567222508 | 0.0248682421281 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 10000.0}                |        15       |
|  0.56352701797  | 0.0414654420087 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.001}               |        7        |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.001}                 |        40       |
|  0.56352701797  | 0.0414654420087 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.01}                |        7        |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.01}                 |        40       |
|  0.56352701797  | 0.0414654420087 |        {'kernel': 'linear', 'C': 100.0, 'gamma': 0.10000000000000001}        |        7        |
|  0.551716605393 |  0.028183811798 |         {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.10000000000000001}          |        30       |
|  0.56352701797  | 0.0414654420087 |                {'kernel': 'linear', 'C': 100.0, 'gamma': 1.0}                |        7        |
|  0.548455949476 | 0.0438425504588 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 1.0}                  |       117       |
|  0.56352701797  | 0.0414654420087 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 10.0}                |        7        |
|  0.541098948388 | 0.0529823914327 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}                 |       122       |
|  0.56352701797  | 0.0414654420087 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 100.0}               |        7        |
|  0.54220640415  | 0.0401614843885 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 100.0}                 |       120       |
|  0.56352701797  | 0.0414654420087 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 1000.0}               |        7        |
|  0.561990894038 | 0.0209745552517 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 1000.0}                |        16       |
|  0.56352701797  | 0.0414654420087 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 10000.0}              |        7        |
|  0.567511491265 | 0.0177840502637 |               {'kernel': 'rbf', 'C': 100.0, 'gamma': 10000.0}                |        4        |
|  0.551435595872 | 0.0492495239561 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.001}               |        31       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.001}                |        40       |
|  0.551435595872 | 0.0492495239561 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.01}               |        31       |
|  0.552025699547 | 0.0330085892569 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.01}                 |        28       |
|  0.551435595872 | 0.0492495239561 |       {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.10000000000000001}        |        31       |
|  0.541698644951 | 0.0351953738669 |         {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.10000000000000001}         |       121       |
|  0.551435595872 | 0.0492495239561 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 1.0}                |        31       |
|  0.522424390277 | 0.0439125141419 |                 {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1.0}                 |       128       |
|  0.551435595872 | 0.0492495239561 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 10.0}               |        31       |
|  0.530741878496 | 0.0497330792932 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10.0}                 |       126       |
|  0.551435595872 | 0.0492495239561 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 100.0}               |        31       |
|  0.554007600922 | 0.0429743369272 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 100.0}                |        27       |
|  0.551435595872 | 0.0492495239561 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 1000.0}              |        31       |
|  0.572905223161 | 0.0144358876547 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}                |        1        |
|  0.551435595872 | 0.0492495239561 |             {'kernel': 'linear', 'C': 1000.0, 'gamma': 10000.0}              |        31       |
|  0.567511491265 | 0.0177840502637 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10000.0}               |        4        |
|  0.561743412351 | 0.0608383521518 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.001}              |        17       |
|  0.552025699547 | 0.0330085892569 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.001}                |        28       |
|  0.561743412351 | 0.0608383521518 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.01}               |        17       |
|  0.541059254845 | 0.0276678719004 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.01}                |       123       |
|  0.561743412351 | 0.0608383521518 |       {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.10000000000000001}       |        17       |
|  0.530221478125 | 0.0448021227185 |        {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.10000000000000001}         |       127       |
|  0.561743412351 | 0.0608383521518 |               {'kernel': 'linear', 'C': 10000.0, 'gamma': 1.0}               |        17       |
|  0.531507983606 | 0.0393869139084 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1.0}                 |       125       |
|  0.561743412351 | 0.0608383521518 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 10.0}               |        17       |
|  0.543586317177 | 0.0499775331387 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10.0}                |       119       |
|  0.561743412351 | 0.0608383521518 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 100.0}              |        17       |
|  0.57207573703  | 0.0437141784169 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 100.0}                |        3        |
|  0.561743412351 | 0.0608383521518 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 1000.0}              |        17       |
|  0.572905223161 | 0.0144358876547 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1000.0}               |        1        |
|  0.561743412351 | 0.0608383521518 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 10000.0}             |        17       |
|  0.567511491265 | 0.0177840502637 |              {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10000.0}               |        4        |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.66      0.97      0.78        91
          1       0.25      0.03      0.05        38
          2       0.00      0.00      0.00         6
          3       0.00      0.00      0.00         3

avg / total       0.50      0.64      0.53       138


Average accuracy on test set (using best parameters): 0.64

===================================================================
[ 0.65671642  0.25        0.          0.        ]
===================================================================

Evaluating DummyClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 1 candidates, totalling 10 fits
Grid scores on validation set:

+-----------------+-----------------+--------+-----------------+
| test_mean_score |  test_std_score | params | test_rank_score |
+-----------------+-----------------+--------+-----------------+
|  0.558494902872 | 0.0186345815588 |   {}   |        1        |
+-----------------+-----------------+--------+-----------------+
Best parameters set found on validation set:

{}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.64      1.00      0.78        88
          1       0.00      0.00      0.00        45
          2       0.00      0.00      0.00         5

avg / total       0.41      0.64      0.50       138


Average accuracy on test set (using best parameters): 0.64

===================================================================
[ 0.63768116  0.          0.        ]
===================================================================
Evaluating KNeighborsClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 5 candidates, totalling 50 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.544291635282 | 0.0487842380332 |  {'n_neighbors': 3} |        1        |
|  0.518385423799 | 0.0595484638592 |  {'n_neighbors': 5} |        5        |
|  0.535984749936 | 0.0404769128614 | {'n_neighbors': 11} |        2        |
|  0.533448267968 | 0.0213524429274 | {'n_neighbors': 21} |        3        |
|  0.524140909547 | 0.0218602872957 | {'n_neighbors': 31} |        4        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.73      0.84      0.78        99
          1       0.36      0.26      0.30        35
          2       0.00      0.00      0.00         3
          3       0.00      0.00      0.00         1

avg / total       0.62      0.67      0.64       138


Average accuracy on test set (using best parameters): 0.67

===================================================================
[ 0.73451327  0.36        0.          0.        ]
===================================================================
Evaluating RandomForestClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 7 candidates, totalling 70 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.511772981074 |  0.029203964638 | {'n_estimators': 2}  |        7        |
|  0.578858720859 | 0.0371230269567 | {'n_estimators': 3}  |        1        |
|  0.552845176916 | 0.0368654176043 | {'n_estimators': 5}  |        4        |
|  0.556219816819 | 0.0602527472202 | {'n_estimators': 10} |        3        |
|  0.552193293088 | 0.0578245353041 | {'n_estimators': 20} |        5        |
|  0.551051348203 | 0.0446121539463 | {'n_estimators': 40} |        6        |
|  0.563877018547 | 0.0459312771071 | {'n_estimators': 60} |        2        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.70      0.78      0.74        95
          1       0.31      0.28      0.29        36
          2       0.00      0.00      0.00         5
          3       0.00      0.00      0.00         2

avg / total       0.56      0.61      0.58       138


Average accuracy on test set (using best parameters): 0.61

===================================================================
[ 0.69811321  0.3125      0.          0.        ]
===================================================================
Evaluating MLPClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 21 candidates, totalling 210 fits
Grid scores on validation set:

+-----------------+-----------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                         params                        | test_rank_score |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
|  0.552258445853 | 0.0240590240792 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        13       |
|  0.552258445853 | 0.0240590240792 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        13       |
|  0.552258445853 | 0.0240590240792 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        13       |
|  0.552258445853 | 0.0240590240792 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        13       |
|  0.552258445853 | 0.0240590240792 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        13       |
|  0.552258445853 | 0.0240590240792 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        13       |
|  0.552258445853 | 0.0240590240792 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        13       |
|  0.55755648692  | 0.0331018531515 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        8        |
|  0.548844466008 | 0.0381327430029 |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        21       |
|  0.560082980483 | 0.0429462977287 |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        6        |
|  0.554570532011 | 0.0421477840043 |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        9        |
|  0.559285778079 | 0.0496323355268 |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        7        |
|  0.563065860633 |  0.052324054872 |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        5        |
|  0.564485518863 | 0.0546427362594 |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        3        |
|  0.552258445853 | 0.0240590240792 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        13       |
|  0.552680146727 | 0.0346616969526 |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        12       |
|  0.55394346077  |  0.033565118917 |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        10       |
|  0.553786555198 | 0.0490942253383 |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        11       |
|  0.568382365975 | 0.0559330788804 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        2        |
|   0.5631228847  | 0.0584189512565 |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        4        |
|  0.573701321027 | 0.0576179593561 |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        1        |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'relu', 'hidden_layer_sizes': 150}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.65      1.00      0.79        90
          1       0.00      0.00      0.00        42
          2       0.00      0.00      0.00         5
          3       0.00      0.00      0.00         1

avg / total       0.43      0.65      0.51       138


Average accuracy on test set (using best parameters): 0.65

===================================================================
[ 0.65217391  0.          0.          0.        ]
===================================================================
Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 128 candidates, totalling 1280 fits
Grid scores on validation set:

+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                                    params                                    | test_rank_score |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
|  0.54913580012  | 0.0255129339654 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.001}               |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.001}                 |        31       |
|  0.54913580012  | 0.0255129339654 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.01}                |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.01}                 |        31       |
|  0.54913580012  | 0.0255129339654 |        {'kernel': 'linear', 'C': 0.001, 'gamma': 0.10000000000000001}        |        31       |
|  0.54913580012  | 0.0255129339654 |         {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.10000000000000001}          |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'linear', 'C': 0.001, 'gamma': 1.0}                |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 1.0}                  |        31       |
|  0.54913580012  | 0.0255129339654 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 10.0}                |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 10.0}                 |        31       |
|  0.54913580012  | 0.0255129339654 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 100.0}               |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 100.0}                 |        31       |
|  0.54913580012  | 0.0255129339654 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 1000.0}               |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 1000.0}                |        31       |
|  0.54913580012  | 0.0255129339654 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 10000.0}              |        31       |
|  0.54913580012  | 0.0255129339654 |               {'kernel': 'rbf', 'C': 0.001, 'gamma': 10000.0}                |        31       |
|  0.54913580012  | 0.0255129339654 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 0.001}                |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.001}                 |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 0.01}                |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.01}                  |        31       |
|  0.54913580012  | 0.0255129339654 |        {'kernel': 'linear', 'C': 0.01, 'gamma': 0.10000000000000001}         |        31       |
|  0.54913580012  | 0.0255129339654 |          {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.10000000000000001}          |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 1.0}                 |        31       |
|  0.54913580012  | 0.0255129339654 |                  {'kernel': 'rbf', 'C': 0.01, 'gamma': 1.0}                  |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 10.0}                |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 10.0}                  |        31       |
|  0.54913580012  | 0.0255129339654 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 100.0}                |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 100.0}                 |        31       |
|  0.54913580012  | 0.0255129339654 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 1000.0}               |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 1000.0}                 |        31       |
|  0.54913580012  | 0.0255129339654 |              {'kernel': 'linear', 'C': 0.01, 'gamma': 10000.0}               |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 10000.0}                |        31       |
|  0.54913580012  | 0.0255129339654 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.001}        |        31       |
|  0.54913580012  | 0.0255129339654 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.001}          |        31       |
|  0.54913580012  | 0.0255129339654 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.01}         |        31       |
|  0.54913580012  | 0.0255129339654 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.01}          |        31       |
|  0.54913580012  | 0.0255129339654 | {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        31       |
|  0.54913580012  | 0.0255129339654 |  {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001}   |        31       |
|  0.54913580012  | 0.0255129339654 |         {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1.0}         |        31       |
|  0.54913580012  | 0.0255129339654 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1.0}           |        31       |
|  0.54913580012  | 0.0255129339654 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10.0}         |        31       |
|  0.54913580012  | 0.0255129339654 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10.0}          |        31       |
|  0.54913580012  | 0.0255129339654 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 100.0}        |        31       |
|  0.54913580012  | 0.0255129339654 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 100.0}          |        31       |
|  0.54913580012  | 0.0255129339654 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1000.0}        |        31       |
|  0.54913580012  | 0.0255129339654 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1000.0}         |        31       |
|  0.54913580012  | 0.0255129339654 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10000.0}       |        31       |
|  0.54913580012  | 0.0255129339654 |        {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10000.0}         |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.001}                |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.001}                  |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.01}                 |        31       |
|  0.54913580012  | 0.0255129339654 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.01}                  |        31       |
|  0.54913580012  | 0.0255129339654 |         {'kernel': 'linear', 'C': 1.0, 'gamma': 0.10000000000000001}         |        31       |
|  0.54913580012  | 0.0255129339654 |          {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.10000000000000001}           |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'linear', 'C': 1.0, 'gamma': 1.0}                 |        31       |
|  0.54913580012  | 0.0255129339654 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 1.0}                   |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 10.0}                 |        31       |
|  0.54913580012  | 0.0255129339654 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 10.0}                  |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 100.0}                |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 100.0}                  |        31       |
|  0.54913580012  | 0.0255129339654 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 1000.0}                |        31       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 1000.0}                 |        31       |
|  0.54913580012  | 0.0255129339654 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 10000.0}               |        31       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'rbf', 'C': 1.0, 'gamma': 10000.0}                 |        31       |
|  0.546748931717 |  0.024423911306 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 0.001}                |       101       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.001}                 |        31       |
|  0.546748931717 |  0.024423911306 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 0.01}                |       101       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.01}                  |        31       |
|  0.546748931717 |  0.024423911306 |        {'kernel': 'linear', 'C': 10.0, 'gamma': 0.10000000000000001}         |       101       |
|  0.54913580012  | 0.0255129339654 |          {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.10000000000000001}          |        31       |
|  0.546748931717 |  0.024423911306 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 1.0}                 |       101       |
|  0.564211156192 | 0.0313426879155 |                  {'kernel': 'rbf', 'C': 10.0, 'gamma': 1.0}                  |        12       |
|  0.546748931717 |  0.024423911306 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 10.0}                |       101       |
|  0.558759353723 |  0.056839770071 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 10.0}                  |        27       |
|  0.546748931717 |  0.024423911306 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 100.0}                |       101       |
|  0.546193803316 | 0.0347369098415 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 100.0}                 |       109       |
|  0.546748931717 |  0.024423911306 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 1000.0}               |       101       |
|  0.554657813244 | 0.0381302889425 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 1000.0}                 |        29       |
|  0.546748931717 |  0.024423911306 |              {'kernel': 'linear', 'C': 10.0, 'gamma': 10000.0}               |       101       |
|  0.564952803737 | 0.0275651901302 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 10000.0}                |        6        |
|  0.562134152462 | 0.0280796836138 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.001}               |        16       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.001}                 |        31       |
|  0.562134152462 | 0.0280796836138 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.01}                |        16       |
|  0.54913580012  | 0.0255129339654 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.01}                 |        31       |
|  0.562134152462 | 0.0280796836138 |        {'kernel': 'linear', 'C': 100.0, 'gamma': 0.10000000000000001}        |        16       |
|  0.561368449607 | 0.0336998591703 |         {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.10000000000000001}          |        24       |
|  0.562134152462 | 0.0280796836138 |                {'kernel': 'linear', 'C': 100.0, 'gamma': 1.0}                |        16       |
|  0.579369030064 |  0.043508486395 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 1.0}                  |        2        |
|  0.562134152462 | 0.0280796836138 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 10.0}                |        16       |
|  0.589345342064 | 0.0594470119986 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}                 |        1        |
|  0.562134152462 | 0.0280796836138 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 100.0}               |        16       |
|  0.54577521026  | 0.0465478875489 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 100.0}                 |       110       |
|  0.562134152462 | 0.0280796836138 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 1000.0}               |        16       |
|  0.555954629416 | 0.0384634559423 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 1000.0}                |        28       |
|  0.562134152462 | 0.0280796836138 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 10000.0}              |        16       |
|  0.562562126264 | 0.0274366388344 |               {'kernel': 'rbf', 'C': 100.0, 'gamma': 10000.0}                |        13       |
|  0.536364964355 | 0.0426373601862 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.001}               |       120       |
|  0.54913580012  | 0.0255129339654 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.001}                |        31       |
|  0.536364964355 | 0.0426373601862 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.01}               |       120       |
|  0.561368449607 | 0.0336998591703 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.01}                 |        24       |
|  0.536364964355 | 0.0426373601862 |       {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.10000000000000001}        |       120       |
|  0.564327273795 | 0.0453161674715 |         {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.10000000000000001}         |        10       |
|  0.536364964355 | 0.0426373601862 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 1.0}                |       120       |
|  0.564651888944 | 0.0941026337689 |                 {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1.0}                 |        7        |
|  0.536364964355 | 0.0426373601862 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 10.0}               |       120       |
|  0.543483684106 | 0.0720806938187 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10.0}                 |       111       |
|  0.536364964355 | 0.0426373601862 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 100.0}               |       120       |
|  0.564312685951 | 0.0633224130765 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 100.0}                |        11       |
|  0.536364964355 | 0.0426373601862 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 1000.0}              |       120       |
|  0.564649061313 | 0.0310965211746 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}                |        8        |
|  0.536364964355 | 0.0426373601862 |             {'kernel': 'linear', 'C': 1000.0, 'gamma': 10000.0}              |       120       |
|  0.562562126264 | 0.0274366388344 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10000.0}               |        13       |
|  0.539357688322 | 0.0725235194117 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.001}              |       112       |
|  0.561368449607 | 0.0336998591703 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.001}                |        24       |
|  0.539357688322 | 0.0725235194117 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.01}               |       112       |
|  0.568181239174 | 0.0281106859926 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.01}                |        5        |
|  0.539357688322 | 0.0725235194117 |       {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.10000000000000001}       |       112       |
|  0.573111612111 | 0.0593902812435 |        {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.10000000000000001}         |        3        |
|  0.539357688322 | 0.0725235194117 |               {'kernel': 'linear', 'C': 10000.0, 'gamma': 1.0}               |       112       |
|  0.528660037628 | 0.0740367012099 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1.0}                 |       128       |
|  0.539357688322 | 0.0725235194117 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 10.0}               |       112       |
|  0.55417939529  | 0.0854037777355 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10.0}                |        30       |
|  0.539357688322 | 0.0725235194117 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 100.0}              |       112       |
|  0.57069283162  | 0.0461406036758 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 100.0}                |        4        |
|  0.539357688322 | 0.0725235194117 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 1000.0}              |       112       |
|  0.564649061313 | 0.0310965211746 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1000.0}               |        8        |
|  0.539357688322 | 0.0725235194117 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 10000.0}             |       112       |
|  0.562562126264 | 0.0274366388344 |              {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10000.0}               |        13       |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.66      0.76      0.70        91
          1       0.23      0.18      0.20        38
          2       0.00      0.00      0.00         5
          3       0.00      0.00      0.00         4

avg / total       0.50      0.55      0.52       138


Average accuracy on test set (using best parameters): 0.55

===================================================================
[ 0.65714286  0.22580645  0.          0.        ]
===================================================================

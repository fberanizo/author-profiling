Evaluating DummyClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 1 candidates, totalling 10 fits
Grid scores on validation set:

+-----------------+-----------------+--------+-----------------+
| test_mean_score |  test_std_score | params | test_rank_score |
+-----------------+-----------------+--------+-----------------+
|  0.533441861097 | 0.0203750287209 |   {}   |        1        |
+-----------------+-----------------+--------+-----------------+
Best parameters set found on validation set:

{}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.70      1.00      0.82        96
          1       0.00      0.00      0.00        36
          2       0.00      0.00      0.00         3
          3       0.00      0.00      0.00         3

avg / total       0.48      0.70      0.57       138


Average accuracy on test set (using best parameters): 0.70

===================================================================
[ 0.69565217  0.          0.          0.        ]
===================================================================
Evaluating KNeighborsClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 5 candidates, totalling 50 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.566210029733 | 0.0776429535591 |  {'n_neighbors': 3} |        2        |
|  0.590545178617 | 0.0587206688558 |  {'n_neighbors': 5} |        1        |
|  0.564961926461 | 0.0531665643037 | {'n_neighbors': 11} |        3        |
|  0.548976728502 | 0.0397293756846 | {'n_neighbors': 21} |        4        |
|  0.547797879251 | 0.0409113692993 | {'n_neighbors': 31} |        5        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.68      0.88      0.76        92
          1       0.28      0.13      0.18        39
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         3

avg / total       0.53      0.62      0.56       138


Average accuracy on test set (using best parameters): 0.62

===================================================================
[ 0.675       0.27777778  0.          0.        ]
===================================================================
Evaluating RandomForestClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 7 candidates, totalling 70 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.565668429375 | 0.0397032700623 | {'n_estimators': 2}  |        1        |
|  0.538384940964 | 0.0674951758928 | {'n_estimators': 3}  |        5        |
|  0.54229894333  | 0.0628892150863 | {'n_estimators': 5}  |        3        |
|  0.552265987089 |  0.039124687715 | {'n_estimators': 10} |        2        |
|  0.534405369964 |  0.038236475377 | {'n_estimators': 20} |        7        |
|  0.535841509569 | 0.0347954112061 | {'n_estimators': 40} |        6        |
|  0.539884154497 | 0.0373378774621 | {'n_estimators': 60} |        4        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 2}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.68      0.77      0.72        95
          1       0.29      0.24      0.26        38
          2       0.00      0.00      0.00         5

avg / total       0.55      0.59      0.57       138


Average accuracy on test set (using best parameters): 0.59

===================================================================
[ 0.68224299  0.29032258  0.        ]
===================================================================
Evaluating MLPClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 21 candidates, totalling 210 fits
Grid scores on validation set:

+-----------------+-----------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                         params                        | test_rank_score |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
|  0.542753569918 | 0.0144676930608 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        13       |
|  0.542753569918 | 0.0144676930608 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        13       |
|  0.542753569918 | 0.0144676930608 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        13       |
|  0.542753569918 | 0.0144676930608 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        13       |
|  0.542753569918 | 0.0144676930608 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        13       |
|  0.542753569918 | 0.0144676930608 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        13       |
|  0.542753569918 | 0.0144676930608 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        13       |
|  0.546859171257 | 0.0224818197394 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        12       |
|  0.550956093411 | 0.0257787652712 |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        10       |
|  0.551412060494 | 0.0355749513365 |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        8        |
|  0.557427548521 | 0.0380910688868 |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        4        |
|  0.554629517787 | 0.0355658635424 |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        7        |
|  0.554846855186 | 0.0387119450754 |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        6        |
|  0.557108811573 |  0.042849476818 |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        5        |
|  0.542753569918 | 0.0144676930608 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        13       |
|  0.542753569918 | 0.0144676930608 |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        13       |
|  0.549415833662 |  0.017369679165 |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        11       |
|  0.558795224974 | 0.0293301568819 |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        3        |
|  0.551365857425 | 0.0350221931084 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        9        |
|  0.571649981069 | 0.0410807798816 |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        1        |
|  0.559930671478 | 0.0384107639976 |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        2        |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'relu', 'hidden_layer_sizes': 120}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      1.00      0.81        93
          1       0.00      0.00      0.00        35
          2       0.00      0.00      0.00         7
          3       0.00      0.00      0.00         3

avg / total       0.45      0.67      0.54       138


Average accuracy on test set (using best parameters): 0.67

===================================================================
[ 0.67391304  0.          0.          0.        ]
===================================================================
Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 128 candidates, totalling 1280 fits
Grid scores on validation set:

+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                                    params                                    | test_rank_score |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
|  0.542846008784 | 0.0238431185988 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.001}               |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.001}                 |        35       |
|  0.542846008784 | 0.0238431185988 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.01}                |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.01}                 |        35       |
|  0.542846008784 | 0.0238431185988 |        {'kernel': 'linear', 'C': 0.001, 'gamma': 0.10000000000000001}        |        35       |
|  0.542846008784 | 0.0238431185988 |         {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.10000000000000001}          |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'linear', 'C': 0.001, 'gamma': 1.0}                |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 1.0}                  |        35       |
|  0.542846008784 | 0.0238431185988 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 10.0}                |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 10.0}                 |        35       |
|  0.542846008784 | 0.0238431185988 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 100.0}               |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 100.0}                 |        35       |
|  0.542846008784 | 0.0238431185988 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 1000.0}               |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 1000.0}                |        35       |
|  0.542846008784 | 0.0238431185988 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 10000.0}              |        35       |
|  0.542846008784 | 0.0238431185988 |               {'kernel': 'rbf', 'C': 0.001, 'gamma': 10000.0}                |        35       |
|  0.542846008784 | 0.0238431185988 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 0.001}                |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.001}                 |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 0.01}                |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.01}                  |        35       |
|  0.542846008784 | 0.0238431185988 |        {'kernel': 'linear', 'C': 0.01, 'gamma': 0.10000000000000001}         |        35       |
|  0.542846008784 | 0.0238431185988 |          {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.10000000000000001}          |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 1.0}                 |        35       |
|  0.542846008784 | 0.0238431185988 |                  {'kernel': 'rbf', 'C': 0.01, 'gamma': 1.0}                  |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 10.0}                |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 10.0}                  |        35       |
|  0.542846008784 | 0.0238431185988 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 100.0}                |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 100.0}                 |        35       |
|  0.542846008784 | 0.0238431185988 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 1000.0}               |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 1000.0}                 |        35       |
|  0.542846008784 | 0.0238431185988 |              {'kernel': 'linear', 'C': 0.01, 'gamma': 10000.0}               |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 10000.0}                |        35       |
|  0.542846008784 | 0.0238431185988 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.001}        |        35       |
|  0.542846008784 | 0.0238431185988 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.001}          |        35       |
|  0.542846008784 | 0.0238431185988 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.01}         |        35       |
|  0.542846008784 | 0.0238431185988 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.01}          |        35       |
|  0.542846008784 | 0.0238431185988 | {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        35       |
|  0.542846008784 | 0.0238431185988 |  {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001}   |        35       |
|  0.542846008784 | 0.0238431185988 |         {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1.0}         |        35       |
|  0.542846008784 | 0.0238431185988 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1.0}           |        35       |
|  0.542846008784 | 0.0238431185988 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10.0}         |        35       |
|  0.542846008784 | 0.0238431185988 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10.0}          |        35       |
|  0.542846008784 | 0.0238431185988 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 100.0}        |        35       |
|  0.542846008784 | 0.0238431185988 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 100.0}          |        35       |
|  0.542846008784 | 0.0238431185988 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1000.0}        |        35       |
|  0.542846008784 | 0.0238431185988 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1000.0}         |        35       |
|  0.542846008784 | 0.0238431185988 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10000.0}       |        35       |
|  0.542846008784 | 0.0238431185988 |        {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10000.0}         |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.001}                |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.001}                  |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.01}                 |        35       |
|  0.542846008784 | 0.0238431185988 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.01}                  |        35       |
|  0.542846008784 | 0.0238431185988 |         {'kernel': 'linear', 'C': 1.0, 'gamma': 0.10000000000000001}         |        35       |
|  0.542846008784 | 0.0238431185988 |          {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.10000000000000001}           |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'linear', 'C': 1.0, 'gamma': 1.0}                 |        35       |
|  0.542846008784 | 0.0238431185988 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 1.0}                   |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 10.0}                 |        35       |
|  0.542846008784 | 0.0238431185988 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 10.0}                  |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 100.0}                |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 100.0}                  |        35       |
|  0.542846008784 | 0.0238431185988 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 1000.0}                |        35       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 1000.0}                 |        35       |
|  0.542846008784 | 0.0238431185988 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 10000.0}               |        35       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'rbf', 'C': 1.0, 'gamma': 10000.0}                 |        35       |
|  0.553387221268 | 0.0242191789249 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 0.001}                |        21       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.001}                 |        35       |
|  0.553387221268 | 0.0242191789249 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 0.01}                |        21       |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.01}                  |        35       |
|  0.553387221268 | 0.0242191789249 |        {'kernel': 'linear', 'C': 10.0, 'gamma': 0.10000000000000001}         |        21       |
|  0.542846008784 | 0.0238431185988 |          {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.10000000000000001}          |        35       |
|  0.553387221268 | 0.0242191789249 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 1.0}                 |        21       |
|  0.557490836323 |  0.032963655756 |                  {'kernel': 'rbf', 'C': 10.0, 'gamma': 1.0}                  |        19       |
|  0.553387221268 | 0.0242191789249 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 10.0}                |        21       |
|  0.561570097387 | 0.0784459066083 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 10.0}                  |        16       |
|  0.553387221268 | 0.0242191789249 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 100.0}                |        21       |
|  0.549221087125 | 0.0407625089528 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 100.0}                 |        30       |
|  0.553387221268 | 0.0242191789249 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 1000.0}               |        21       |
|  0.54573006406  | 0.0232416935393 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 1000.0}                 |        31       |
|  0.553387221268 | 0.0242191789249 |              {'kernel': 'linear', 'C': 10.0, 'gamma': 10000.0}               |        21       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 10000.0}                |        35       |
|  0.569988976989 | 0.0349087045248 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.001}               |        4        |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.001}                 |        35       |
|  0.569988976989 | 0.0349087045248 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.01}                |        4        |
|  0.542846008784 | 0.0238431185988 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.01}                 |        35       |
|  0.569988976989 | 0.0349087045248 |        {'kernel': 'linear', 'C': 100.0, 'gamma': 0.10000000000000001}        |        4        |
|  0.558646702543 |  0.020117483788 |         {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.10000000000000001}          |        18       |
|  0.569988976989 | 0.0349087045248 |                {'kernel': 'linear', 'C': 100.0, 'gamma': 1.0}                |        4        |
|  0.577365821753 | 0.0319464522653 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 1.0}                  |        2        |
|  0.569988976989 | 0.0349087045248 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 10.0}                |        4        |
|  0.537034686373 | 0.0605209989999 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}                 |       117       |
|  0.569988976989 | 0.0349087045248 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 100.0}               |        4        |
|  0.551038091522 | 0.0432838170304 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 100.0}                 |        29       |
|  0.569988976989 | 0.0349087045248 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 1000.0}               |        4        |
|  0.542980316326 | 0.0263734163377 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 1000.0}                |        34       |
|  0.569988976989 | 0.0349087045248 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 10000.0}              |        4        |
|  0.541667159532 |   0.0254824795  |               {'kernel': 'rbf', 'C': 100.0, 'gamma': 10000.0}                |       106       |
|  0.53609221016  | 0.0514968702775 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.001}               |       118       |
|  0.542846008784 | 0.0238431185988 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.001}                |        35       |
|  0.53609221016  | 0.0514968702775 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.01}               |       118       |
|  0.562750317598 | 0.0293527060743 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.01}                 |        15       |
|  0.53609221016  | 0.0514968702775 |       {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.10000000000000001}        |       118       |
|  0.578050634252 | 0.0281671293583 |         {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.10000000000000001}         |        1        |
|  0.53609221016  | 0.0514968702775 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 1.0}                |       118       |
|  0.52908638797  | 0.0705708955777 |                 {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1.0}                 |       126       |
|  0.53609221016  | 0.0514968702775 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 10.0}               |       118       |
|  0.565041298588 | 0.0821676432845 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10.0}                 |        13       |
|  0.53609221016  | 0.0514968702775 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 100.0}               |       118       |
|  0.556343446372 | 0.0381184502423 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 100.0}                |        20       |
|  0.53609221016  | 0.0514968702775 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 1000.0}              |       118       |
|  0.543659506628 | 0.0253126070952 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}                |        32       |
|  0.53609221016  | 0.0514968702775 |             {'kernel': 'linear', 'C': 1000.0, 'gamma': 10000.0}              |       118       |
|  0.541667159532 |   0.0254824795  |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10000.0}               |       106       |
|  0.537825615439 |  0.049403684885 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.001}              |       109       |
|  0.56394474361  |  0.02922188741  |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.001}                |        14       |
|  0.537825615439 |  0.049403684885 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.01}               |       109       |
|  0.566415268958 |  0.02992895617  |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.01}                |        12       |
|  0.537825615439 |  0.049403684885 |       {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.10000000000000001}       |       109       |
|  0.524243586587 | 0.0473488343816 |        {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.10000000000000001}         |       127       |
|  0.537825615439 |  0.049403684885 |               {'kernel': 'linear', 'C': 10000.0, 'gamma': 1.0}               |       109       |
|  0.502350062059 | 0.0502284358301 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1.0}                 |       128       |
|  0.537825615439 |  0.049403684885 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 10.0}               |       109       |
|  0.576153507575 | 0.0737026717722 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10.0}                |        3        |
|  0.537825615439 |  0.049403684885 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 100.0}              |       109       |
|  0.559587872846 | 0.0260844682574 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 100.0}                |        17       |
|  0.537825615439 |  0.049403684885 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 1000.0}              |       109       |
|  0.543659506628 | 0.0253126070952 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1000.0}               |        32       |
|  0.537825615439 |  0.049403684885 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 10000.0}             |       109       |
|  0.541667159532 |   0.0254824795  |              {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10000.0}               |       106       |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.10000000000000001}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.66      0.87      0.75        93
          1       0.13      0.05      0.08        38
          2       0.00      0.00      0.00         5
          3       0.00      0.00      0.00         2

avg / total       0.48      0.60      0.53       138


Average accuracy on test set (using best parameters): 0.60

===================================================================
[ 0.65853659  0.13333333  0.          0.        ]
===================================================================

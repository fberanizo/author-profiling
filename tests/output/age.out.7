Evaluating DummyClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 1 candidates, totalling 10 fits
Grid scores on validation set:

+-----------------+-----------------+--------+-----------------+
| test_mean_score |  test_std_score | params | test_rank_score |
+-----------------+-----------------+--------+-----------------+
|  0.552236195918 | 0.0222536704486 |   {}   |        1        |
+-----------------+-----------------+--------+-----------------+
Best parameters set found on validation set:

{}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.65      1.00      0.79        90
          1       0.00      0.00      0.00        40
          2       0.00      0.00      0.00         7
          3       0.00      0.00      0.00         1

avg / total       0.43      0.65      0.51       138


Average accuracy on test set (using best parameters): 0.65

===================================================================
[ 0.65217391  0.          0.          0.        ]
===================================================================
Evaluating KNeighborsClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 5 candidates, totalling 50 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.570168289116 | 0.0474892552916 |  {'n_neighbors': 3} |        2        |
|  0.571883405685 | 0.0418875478603 |  {'n_neighbors': 5} |        1        |
|  0.566709618262 | 0.0458886803506 | {'n_neighbors': 11} |        3        |
|  0.560616057841 | 0.0240272044465 | {'n_neighbors': 21} |        4        |
|  0.555342288543 | 0.0183775851169 | {'n_neighbors': 31} |        5        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.66      0.96      0.78        89
          1       0.50      0.11      0.19        44
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         1

avg / total       0.59      0.65      0.56       138


Average accuracy on test set (using best parameters): 0.65

===================================================================
[ 0.6640625  0.5        0.         0.       ]
===================================================================
Evaluating RandomForestClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 7 candidates, totalling 70 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.602848040575 | 0.0513018645131 | {'n_estimators': 2}  |        2        |
|  0.569854328019 | 0.0579523350121 | {'n_estimators': 3}  |        6        |
|  0.604417381824 | 0.0533427254614 | {'n_estimators': 5}  |        1        |
|  0.54988825599  |  0.029267310092 | {'n_estimators': 10} |        7        |
|  0.574935442093 |  0.019586306235 | {'n_estimators': 20} |        5        |
|  0.601044415837 | 0.0525516439067 | {'n_estimators': 40} |        3        |
|  0.581158056028 | 0.0255539554533 | {'n_estimators': 60} |        4        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.57      0.89      0.70        79
          1       0.40      0.11      0.17        54
          2       0.00      0.00      0.00         5
          3       0.00      0.00      0.00         0

avg / total       0.48      0.55      0.47       138


Average accuracy on test set (using best parameters): 0.55

===================================================================
[ 0.57377049  0.4         0.          0.        ]
===================================================================
Evaluating MLPClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 21 candidates, totalling 210 fits
Grid scores on validation set:

+-----------------+-----------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                         params                        | test_rank_score |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
|  0.539666509458 | 0.0190907588344 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        9        |
|  0.539666509458 | 0.0190907588344 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        9        |
|  0.539666509458 | 0.0190907588344 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        9        |
|  0.539666509458 | 0.0190907588344 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        9        |
|  0.539666509458 | 0.0190907588344 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        9        |
|  0.539666509458 | 0.0190907588344 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        9        |
|  0.539666509458 | 0.0190907588344 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        9        |
|  0.539666509458 | 0.0190907588344 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        9        |
|  0.544948240668 | 0.0232948300075 |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        2        |
|  0.542584582361 | 0.0257978798996 |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        6        |
|  0.539884634788 | 0.0237360248219 |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        8        |
|  0.542777513974 | 0.0257301614892 |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        5        |
|  0.540438768986 | 0.0382069585689 |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        7        |
|  0.539244342974 | 0.0375670339275 |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        19       |
|  0.539666509458 | 0.0190907588344 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        9        |
|  0.539666509458 | 0.0190907588344 |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        9        |
|  0.538487660206 | 0.0209242258883 |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        20       |
|  0.538487660206 | 0.0209242258883 |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        20       |
|  0.547862639032 | 0.0280776718628 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        1        |
|  0.543971939986 | 0.0264969928961 |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        3        |
|  0.543971939986 | 0.0264969928961 |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        3        |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'relu', 'hidden_layer_sizes': 100}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.68      1.00      0.81        94
          1       0.00      0.00      0.00        38
          2       0.00      0.00      0.00         6

avg / total       0.46      0.68      0.55       138


Average accuracy on test set (using best parameters): 0.68

===================================================================
[ 0.68115942  0.          0.        ]
===================================================================
Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 128 candidates, totalling 1280 fits
Grid scores on validation set:

+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                                    params                                    | test_rank_score |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.001}               |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.001}                 |        18       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.01}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.01}                 |        18       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.001, 'gamma': 0.10000000000000001}        |        18       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.10000000000000001}          |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 0.001, 'gamma': 1.0}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 1.0}                  |        18       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 10.0}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 10.0}                 |        18       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 100.0}               |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 100.0}                 |        18       |
|  0.549113550185 | 0.0238151223172 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 1000.0}               |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 1000.0}                |        18       |
|  0.549113550185 | 0.0238151223172 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 10000.0}              |        18       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'rbf', 'C': 0.001, 'gamma': 10000.0}                |        18       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 0.001}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.001}                 |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 0.01}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.01}                  |        18       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.01, 'gamma': 0.10000000000000001}         |        18       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.10000000000000001}          |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 1.0}                 |        18       |
|  0.549113550185 | 0.0238151223172 |                  {'kernel': 'rbf', 'C': 0.01, 'gamma': 1.0}                  |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 10.0}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 10.0}                  |        18       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 100.0}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 100.0}                 |        18       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 1000.0}               |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 1000.0}                 |        18       |
|  0.549113550185 | 0.0238151223172 |              {'kernel': 'linear', 'C': 0.01, 'gamma': 10000.0}               |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 10000.0}                |        18       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.001}        |        18       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.001}          |        18       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.01}         |        18       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.01}          |        18       |
|  0.549113550185 | 0.0238151223172 | {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        18       |
|  0.549113550185 | 0.0238151223172 |  {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001}   |        18       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1.0}         |        18       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1.0}           |        18       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10.0}         |        18       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10.0}          |        18       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 100.0}        |        18       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 100.0}          |        18       |
|  0.549113550185 | 0.0238151223172 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1000.0}        |        18       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1000.0}         |        18       |
|  0.549113550185 | 0.0238151223172 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10000.0}       |        18       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10000.0}         |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.001}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.001}                  |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.01}                 |        18       |
|  0.549113550185 | 0.0238151223172 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.01}                  |        18       |
|  0.549113550185 | 0.0238151223172 |         {'kernel': 'linear', 'C': 1.0, 'gamma': 0.10000000000000001}         |        18       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.10000000000000001}           |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'linear', 'C': 1.0, 'gamma': 1.0}                 |        18       |
|  0.549113550185 | 0.0238151223172 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 1.0}                   |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 10.0}                 |        18       |
|  0.549113550185 | 0.0238151223172 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 10.0}                  |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 100.0}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 100.0}                  |        18       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 1000.0}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 1000.0}                 |        18       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 10000.0}               |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 1.0, 'gamma': 10000.0}                 |        18       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 0.001}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.001}                 |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 0.01}                |        18       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.01}                  |        18       |
|  0.549113550185 | 0.0238151223172 |        {'kernel': 'linear', 'C': 10.0, 'gamma': 0.10000000000000001}         |        18       |
|  0.549113550185 | 0.0238151223172 |          {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.10000000000000001}          |        18       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 1.0}                 |        18       |
|  0.546732316299 | 0.0237782062207 |                  {'kernel': 'rbf', 'C': 10.0, 'gamma': 1.0}                  |       115       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 10.0}                |        18       |
|  0.544140282695 | 0.0446955561955 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 10.0}                  |       116       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 100.0}                |        18       |
|  0.552944215967 | 0.0382517121815 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 100.0}                 |        13       |
|  0.549113550185 | 0.0238151223172 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 1000.0}               |        18       |
|  0.567769201736 | 0.0332161569208 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 1000.0}                 |        1        |
|  0.549113550185 | 0.0238151223172 |              {'kernel': 'linear', 'C': 10.0, 'gamma': 10000.0}               |        18       |
|  0.563391682414 | 0.0258544462416 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 10000.0}                |        4        |
|  0.547366018104 | 0.0511087251627 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.001}               |       107       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.001}                 |        18       |
|  0.547366018104 | 0.0511087251627 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.01}                |       107       |
|  0.549113550185 | 0.0238151223172 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.01}                 |        18       |
|  0.547366018104 | 0.0511087251627 |        {'kernel': 'linear', 'C': 100.0, 'gamma': 0.10000000000000001}        |       107       |
|  0.547922933242 | 0.0238264378314 |         {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.10000000000000001}          |       104       |
|  0.547366018104 | 0.0511087251627 |                {'kernel': 'linear', 'C': 100.0, 'gamma': 1.0}                |       107       |
|  0.550706356289 |  0.041133061535 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 1.0}                  |        16       |
|  0.547366018104 | 0.0511087251627 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 10.0}                |       107       |
|  0.554841837802 | 0.0544013519321 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}                 |        9        |
|  0.547366018104 | 0.0511087251627 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 100.0}               |       107       |
|  0.552403737514 | 0.0398172417509 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 100.0}                 |        14       |
|  0.547366018104 | 0.0511087251627 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 1000.0}               |       107       |
|  0.560304635941 | 0.0252354251625 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 1000.0}                |        8        |
|  0.547366018104 | 0.0511087251627 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 10000.0}              |       107       |
|  0.563363593483 | 0.0246134236351 |               {'kernel': 'rbf', 'C': 100.0, 'gamma': 10000.0}                |        5        |
|  0.548011154697 | 0.0914520468524 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.001}               |        96       |
|  0.549113550185 | 0.0238151223172 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.001}                |        18       |
|  0.548011154697 | 0.0914520468524 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.01}               |        96       |
|  0.547922933242 | 0.0238264378314 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.01}                 |       104       |
|  0.548011154697 | 0.0914520468524 |       {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.10000000000000001}        |        96       |
|  0.551943549802 |  0.042257609927 |         {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.10000000000000001}         |        15       |
|  0.548011154697 | 0.0914520468524 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 1.0}                |        96       |
|  0.534711547549 | 0.0605096687142 |                 {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1.0}                 |       118       |
|  0.548011154697 | 0.0914520468524 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 10.0}               |        96       |
|   0.554283169   | 0.0724348361319 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10.0}                 |        12       |
|  0.548011154697 | 0.0914520468524 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 100.0}               |        96       |
|  0.554463192068 |  0.057760341033 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 100.0}                |        11       |
|  0.548011154697 | 0.0914520468524 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 1000.0}              |        96       |
|  0.563512960422 | 0.0272909814788 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}                |        2        |
|  0.548011154697 | 0.0914520468524 |             {'kernel': 'linear', 'C': 1000.0, 'gamma': 10000.0}              |        96       |
|  0.563363593483 | 0.0246134236351 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10000.0}               |        5        |
|  0.512519427821 | 0.0844980993256 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.001}              |       121       |
|  0.547922933242 | 0.0238264378314 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.001}                |       104       |
|  0.512519427821 | 0.0844980993256 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.01}               |       121       |
|  0.554518296629 | 0.0584763333325 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.01}                |        10       |
|  0.512519427821 | 0.0844980993256 |       {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.10000000000000001}       |       121       |
|  0.534164729592 | 0.0735621932952 |        {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.10000000000000001}         |       119       |
|  0.512519427821 | 0.0844980993256 |               {'kernel': 'linear', 'C': 10000.0, 'gamma': 1.0}               |       121       |
|  0.518464921481 | 0.0629722077056 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1.0}                 |       120       |
|  0.512519427821 | 0.0844980993256 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 10.0}               |       121       |
|  0.537183514415 | 0.0487634700638 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10.0}                |       117       |
|  0.512519427821 | 0.0844980993256 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 100.0}              |       121       |
|  0.550318931252 | 0.0742928458009 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 100.0}                |        17       |
|  0.512519427821 | 0.0844980993256 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 1000.0}              |       121       |
|  0.563512960422 | 0.0272909814788 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1000.0}               |        2        |
|  0.512519427821 | 0.0844980993256 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 10000.0}             |       121       |
|  0.563363593483 | 0.0246134236351 |              {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10000.0}               |        5        |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'kernel': 'rbf', 'C': 10.0, 'gamma': 1000.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.65      0.98      0.78        91
          1       0.00      0.00      0.00        42
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         1

avg / total       0.43      0.64      0.52       138


Average accuracy on test set (using best parameters): 0.64

===================================================================
[ 0.65441176  0.          0.          0.        ]
===================================================================

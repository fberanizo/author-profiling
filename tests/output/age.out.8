Evaluating DummyClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 1 candidates, totalling 10 fits
Grid scores on validation set:

+-----------------+-----------------+--------+-----------------+
| test_mean_score |  test_std_score | params | test_rank_score |
+-----------------+-----------------+--------+-----------------+
|  0.542868258719 | 0.0255445289218 |   {}   |        1        |
+-----------------+-----------------+--------+-----------------+
Best parameters set found on validation set:

{}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      1.00      0.81        93
          1       0.00      0.00      0.00        40
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         1

avg / total       0.45      0.67      0.54       138


Average accuracy on test set (using best parameters): 0.67

===================================================================
[ 0.67391304  0.          0.          0.        ]
===================================================================
Evaluating KNeighborsClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 5 candidates, totalling 50 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.629062937414 |  0.052091380581 |  {'n_neighbors': 3} |        1        |
|  0.58953584771  | 0.0505779385396 |  {'n_neighbors': 5} |        2        |
|  0.582675883775 | 0.0417270020174 | {'n_neighbors': 11} |        3        |
|  0.574292482184 | 0.0143095366197 | {'n_neighbors': 21} |        4        |
|  0.574292482184 | 0.0143095366197 | {'n_neighbors': 31} |        4        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.58      0.76      0.66        83
          1       0.21      0.13      0.16        46
          2       0.00      0.00      0.00         8
          3       0.00      0.00      0.00         1

avg / total       0.42      0.50      0.45       138


Average accuracy on test set (using best parameters): 0.50

===================================================================
[ 0.57798165  0.20689655  0.          0.        ]
===================================================================
Evaluating RandomForestClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 7 candidates, totalling 70 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.540461164929 | 0.0318632971614 | {'n_estimators': 2}  |        6        |
|  0.559918952276 | 0.0596549885757 | {'n_estimators': 3}  |        4        |
|  0.574288263001 | 0.0672732805008 | {'n_estimators': 5}  |        1        |
|  0.558677543849 | 0.0483362002848 | {'n_estimators': 10} |        5        |
|  0.534801427422 | 0.0439501238496 | {'n_estimators': 20} |        7        |
|  0.566490786043 | 0.0402535412156 | {'n_estimators': 40} |        2        |
|  0.563028107812 | 0.0393349964451 | {'n_estimators': 60} |        3        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.68      0.92      0.78        92
          1       0.38      0.13      0.20        38
          2       0.00      0.00      0.00         7
          3       0.00      0.00      0.00         1

avg / total       0.56      0.65      0.58       138


Average accuracy on test set (using best parameters): 0.65

===================================================================
[ 0.68        0.38461538  0.          0.        ]
===================================================================
Evaluating MLPClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 21 candidates, totalling 210 fits
Grid scores on validation set:

+-----------------+-----------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                         params                        | test_rank_score |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
|  0.539666509458 | 0.0190907588344 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        10       |
|  0.539666509458 | 0.0190907588344 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        10       |
|  0.539666509458 | 0.0190907588344 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        10       |
|  0.539666509458 | 0.0190907588344 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        10       |
|  0.539666509458 | 0.0190907588344 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        10       |
|  0.539666509458 | 0.0190907588344 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        10       |
|  0.539666509458 | 0.0190907588344 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        10       |
|  0.539666509458 | 0.0190907588344 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        10       |
|  0.537293234194 | 0.0196130740551 |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        21       |
|  0.542734245743 |  0.025282883238 |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        5        |
|  0.549933153687 | 0.0255436619922 |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        2        |
|  0.55022436007  | 0.0286508264081 |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        1        |
|  0.54253341809  | 0.0301866388986 |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        6        |
|  0.539982296031 | 0.0299412654524 |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        9        |
|  0.538472083446 | 0.0177235045677 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        20       |
|  0.539666509458 | 0.0190907588344 |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        10       |
|  0.54373156472  | 0.0170046733723 |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        4        |
|  0.541057541621 | 0.0208256089107 |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        7        |
|  0.540041536971 | 0.0241419854943 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        8        |
|  0.538847110959 | 0.0230953867716 |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        19       |
|  0.547804124651 | 0.0291773304463 |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        3        |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'tanh', 'hidden_layer_sizes': 100}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.68      1.00      0.81        94
          1       0.00      0.00      0.00        37
          2       0.00      0.00      0.00         6
          3       0.00      0.00      0.00         1

avg / total       0.46      0.68      0.55       138


Average accuracy on test set (using best parameters): 0.68

===================================================================
[ 0.68115942  0.          0.          0.        ]
===================================================================
Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 128 candidates, totalling 1280 fits
Grid scores on validation set:

+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                                    params                                    | test_rank_score |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.001}               |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.001}                 |        28       |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.01}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.01}                 |        28       |
|  0.533441861097 | 0.0203750287209 |        {'kernel': 'linear', 'C': 0.001, 'gamma': 0.10000000000000001}        |        28       |
|  0.533441861097 | 0.0203750287209 |         {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.10000000000000001}          |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 0.001, 'gamma': 1.0}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 1.0}                  |        28       |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 10.0}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 10.0}                 |        28       |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 100.0}               |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 100.0}                 |        28       |
|  0.533441861097 | 0.0203750287209 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 1000.0}               |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 1000.0}                |        28       |
|  0.533441861097 | 0.0203750287209 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 10000.0}              |        28       |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'rbf', 'C': 0.001, 'gamma': 10000.0}                |        28       |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 0.001}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.001}                 |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 0.01}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.01}                  |        28       |
|  0.533441861097 | 0.0203750287209 |        {'kernel': 'linear', 'C': 0.01, 'gamma': 0.10000000000000001}         |        28       |
|  0.533441861097 | 0.0203750287209 |          {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.10000000000000001}          |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 1.0}                 |        28       |
|  0.533441861097 | 0.0203750287209 |                  {'kernel': 'rbf', 'C': 0.01, 'gamma': 1.0}                  |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 10.0}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 10.0}                  |        28       |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 100.0}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 100.0}                 |        28       |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 1000.0}               |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 1000.0}                 |        28       |
|  0.533441861097 | 0.0203750287209 |              {'kernel': 'linear', 'C': 0.01, 'gamma': 10000.0}               |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 10000.0}                |        28       |
|  0.533441861097 | 0.0203750287209 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.001}        |        28       |
|  0.533441861097 | 0.0203750287209 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.001}          |        28       |
|  0.533441861097 | 0.0203750287209 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.01}         |        28       |
|  0.533441861097 | 0.0203750287209 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.01}          |        28       |
|  0.533441861097 | 0.0203750287209 | {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        28       |
|  0.533441861097 | 0.0203750287209 |  {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001}   |        28       |
|  0.533441861097 | 0.0203750287209 |         {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1.0}         |        28       |
|  0.533441861097 | 0.0203750287209 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1.0}           |        28       |
|  0.533441861097 | 0.0203750287209 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10.0}         |        28       |
|  0.533441861097 | 0.0203750287209 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10.0}          |        28       |
|  0.533441861097 | 0.0203750287209 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 100.0}        |        28       |
|  0.533441861097 | 0.0203750287209 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 100.0}          |        28       |
|  0.533441861097 | 0.0203750287209 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1000.0}        |        28       |
|  0.533441861097 | 0.0203750287209 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1000.0}         |        28       |
|  0.533441861097 | 0.0203750287209 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10000.0}       |        28       |
|  0.533441861097 | 0.0203750287209 |        {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10000.0}         |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.001}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.001}                  |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.01}                 |        28       |
|  0.533441861097 | 0.0203750287209 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.01}                  |        28       |
|  0.533441861097 | 0.0203750287209 |         {'kernel': 'linear', 'C': 1.0, 'gamma': 0.10000000000000001}         |        28       |
|  0.533441861097 | 0.0203750287209 |          {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.10000000000000001}           |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'linear', 'C': 1.0, 'gamma': 1.0}                 |        28       |
|  0.533441861097 | 0.0203750287209 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 1.0}                   |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 10.0}                 |        28       |
|  0.533441861097 | 0.0203750287209 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 10.0}                  |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 100.0}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 100.0}                  |        28       |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 1000.0}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 1000.0}                 |        28       |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 10000.0}               |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'rbf', 'C': 1.0, 'gamma': 10000.0}                 |        28       |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 0.001}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.001}                 |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 0.01}                |        28       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.01}                  |        28       |
|  0.533441861097 | 0.0203750287209 |        {'kernel': 'linear', 'C': 10.0, 'gamma': 0.10000000000000001}         |        28       |
|  0.533441861097 | 0.0203750287209 |          {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.10000000000000001}          |        28       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 1.0}                 |        28       |
|  0.532253377503 |  0.020076736944 |                  {'kernel': 'rbf', 'C': 10.0, 'gamma': 1.0}                  |       108       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 10.0}                |        28       |
|  0.551843447737 |  0.054070914528 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 10.0}                  |        4        |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 100.0}                |        28       |
|  0.552288192785 | 0.0279161011065 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 100.0}                 |        3        |
|  0.533441861097 | 0.0203750287209 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 1000.0}               |        28       |
|  0.536330492379 | 0.0221487295659 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 1000.0}                 |        22       |
|  0.533441861097 | 0.0203750287209 |              {'kernel': 'linear', 'C': 10.0, 'gamma': 10000.0}               |        28       |
|  0.532253377503 |  0.020076736944 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 10000.0}                |       108       |
|  0.517234960533 | 0.0286860846622 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.001}               |       112       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.001}                 |        28       |
|  0.517234960533 | 0.0286860846622 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.01}                |       112       |
|  0.533441861097 | 0.0203750287209 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.01}                 |        28       |
|  0.517234960533 | 0.0286860846622 |        {'kernel': 'linear', 'C': 100.0, 'gamma': 0.10000000000000001}        |       112       |
|  0.533441861097 | 0.0203750287209 |         {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.10000000000000001}          |        28       |
|  0.517234960533 | 0.0286860846622 |                {'kernel': 'linear', 'C': 100.0, 'gamma': 1.0}                |       112       |
|  0.535176992406 | 0.0660802896247 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 1.0}                  |        27       |
|  0.517234960533 | 0.0286860846622 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 10.0}                |       112       |
|  0.542973147661 |  0.056629784903 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}                 |        9        |
|  0.517234960533 | 0.0286860846622 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 100.0}               |       112       |
|  0.53775641381  | 0.0447689058988 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 100.0}                 |        20       |
|  0.517234960533 | 0.0286860846622 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 1000.0}               |       112       |
|  0.537628095727 | 0.0213631963887 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 1000.0}                |        21       |
|  0.517234960533 | 0.0286860846622 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 10000.0}              |       112       |
|  0.536287806458 |  0.021711702923 |               {'kernel': 'rbf', 'C': 100.0, 'gamma': 10000.0}                |        23       |
|  0.539482723234 | 0.0571855227658 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.001}               |        10       |
|  0.533441861097 | 0.0203750287209 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.001}                |        28       |
|  0.539482723234 | 0.0571855227658 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.01}               |        10       |
|  0.538729436458 | 0.0321047454797 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.01}                 |        18       |
|  0.539482723234 | 0.0571855227658 |       {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.10000000000000001}        |        10       |
|  0.512438794172 |  0.049694672258 |         {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.10000000000000001}         |       128       |
|  0.539482723234 | 0.0571855227658 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 1.0}                |        10       |
|  0.560435105209 | 0.0823427248278 |                 {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1.0}                 |        1        |
|  0.539482723234 | 0.0571855227658 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 10.0}               |        10       |
|  0.533010307506 | 0.0422180778333 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10.0}                 |       107       |
|  0.539482723234 | 0.0571855227658 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 100.0}               |        10       |
|  0.548073556196 | 0.0457383886404 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 100.0}                |        8        |
|  0.539482723234 | 0.0571855227658 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 1000.0}              |        10       |
|  0.548519477537 |  0.030734488404 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}                |        6        |
|  0.539482723234 | 0.0571855227658 |             {'kernel': 'linear', 'C': 1000.0, 'gamma': 10000.0}              |        10       |
|  0.536287806458 |  0.021711702923 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10000.0}               |        23       |
|  0.516871329702 | 0.0645784064763 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.001}              |       120       |
|  0.538729436458 | 0.0321047454797 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.001}                |        18       |
|  0.516871329702 | 0.0645784064763 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.01}               |       120       |
|  0.519495904794 | 0.0408995430512 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.01}                |       111       |
|  0.516871329702 | 0.0645784064763 |       {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.10000000000000001}       |       120       |
|  0.535980604396 | 0.0933108838771 |        {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.10000000000000001}         |        26       |
|  0.516871329702 | 0.0645784064763 |               {'kernel': 'linear', 'C': 10000.0, 'gamma': 1.0}               |       120       |
|  0.519816268358 | 0.0658437833271 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1.0}                 |       110       |
|  0.516871329702 | 0.0645784064763 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 10.0}               |       120       |
|  0.549226527547 | 0.0497963098968 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10.0}                |        5        |
|  0.516871329702 | 0.0645784064763 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 100.0}              |       120       |
|  0.559321563277 |  0.042675915572 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 100.0}                |        2        |
|  0.516871329702 | 0.0645784064763 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 1000.0}              |       120       |
|  0.548519477537 |  0.030734488404 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1000.0}               |        6        |
|  0.516871329702 | 0.0645784064763 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 10000.0}             |       120       |
|  0.536287806458 |  0.021711702923 |              {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10000.0}               |        23       |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'kernel': 'rbf', 'C': 1000.0, 'gamma': 1.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.72      0.76      0.74        96
          1       0.29      0.28      0.29        36
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         2

avg / total       0.58      0.60      0.59       138


Average accuracy on test set (using best parameters): 0.60

===================================================================
[ 0.72277228  0.29411765  0.          0.        ]
===================================================================

Evaluating DummyClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 1 candidates, totalling 10 fits
Grid scores on validation set:

+-----------------+----------------+--------+-----------------+
| test_mean_score | test_std_score | params | test_rank_score |
+-----------------+----------------+--------+-----------------+
|  0.530388049794 | 0.025182313332 |   {}   |        1        |
+-----------------+----------------+--------+-----------------+
Best parameters set found on validation set:

{}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.70      1.00      0.83        97
          1       0.00      0.00      0.00        38
          2       0.00      0.00      0.00         3

avg / total       0.49      0.70      0.58       138


Average accuracy on test set (using best parameters): 0.70

===================================================================
[ 0.70289855  0.          0.        ]
===================================================================
Evaluating KNeighborsClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 5 candidates, totalling 50 fits
Grid scores on validation set:

+-----------------+-----------------+---------------------+-----------------+
| test_mean_score |  test_std_score |        params       | test_rank_score |
+-----------------+-----------------+---------------------+-----------------+
|  0.567000395146 | 0.0636562213199 |  {'n_neighbors': 3} |        1        |
|  0.560852656379 | 0.0430737803373 |  {'n_neighbors': 5} |        2        |
|  0.558457598555 | 0.0408314765672 | {'n_neighbors': 11} |        3        |
|  0.547343660607 |  0.022650594831 | {'n_neighbors': 21} |        4        |
|  0.542788173576 | 0.0185002398142 | {'n_neighbors': 31} |        5        |
+-----------------+-----------------+---------------------+-----------------+
Best parameters set found on validation set:

{'n_neighbors': 3}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.67      0.86      0.75        93
          1       0.18      0.08      0.11        36
          2       0.00      0.00      0.00         6
          3       0.00      0.00      0.00         3

avg / total       0.50      0.60      0.54       138


Average accuracy on test set (using best parameters): 0.60

===================================================================
[ 0.66666667  0.17647059  0.          0.        ]
===================================================================
Evaluating RandomForestClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 7 candidates, totalling 70 fits
Grid scores on validation set:

+-----------------+-----------------+----------------------+-----------------+
| test_mean_score |  test_std_score |        params        | test_rank_score |
+-----------------+-----------------+----------------------+-----------------+
|  0.552656571257 | 0.0506015136452 | {'n_estimators': 2}  |        6        |
|  0.54770461094  |  0.079194546604 | {'n_estimators': 3}  |        7        |
|  0.59282166896  | 0.0583168841074 | {'n_estimators': 5}  |        1        |
|  0.566574972395 | 0.0666452407065 | {'n_estimators': 10} |        4        |
|  0.576491364419 | 0.0542771486366 | {'n_estimators': 20} |        2        |
|  0.570734693717 |  0.02951178896  | {'n_estimators': 40} |        3        |
|  0.557128842932 | 0.0424954524718 | {'n_estimators': 60} |        5        |
+-----------------+-----------------+----------------------+-----------------+
Best parameters set found on validation set:

{'n_estimators': 5}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.65      0.79      0.71        90
          1       0.29      0.20      0.23        41
          2       0.00      0.00      0.00         5
          3       0.00      0.00      0.00         2

avg / total       0.51      0.57      0.53       138


Average accuracy on test set (using best parameters): 0.57

===================================================================
[ 0.64545455  0.28571429  0.          0.        ]
===================================================================
Evaluating MLPClassifier
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 21 candidates, totalling 210 fits
Grid scores on validation set:

+-----------------+-----------------+-------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                         params                        | test_rank_score |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
|  0.530341885869 | 0.0214106729429 |  {'activation': 'logistic', 'hidden_layer_sizes': 20} |        13       |
|  0.530341885869 | 0.0214106729429 |  {'activation': 'logistic', 'hidden_layer_sizes': 30} |        13       |
|  0.530341885869 | 0.0214106729429 |  {'activation': 'logistic', 'hidden_layer_sizes': 50} |        13       |
|  0.530341885869 | 0.0214106729429 |  {'activation': 'logistic', 'hidden_layer_sizes': 75} |        13       |
|  0.530341885869 | 0.0214106729429 | {'activation': 'logistic', 'hidden_layer_sizes': 100} |        13       |
|  0.530341885869 | 0.0214106729429 | {'activation': 'logistic', 'hidden_layer_sizes': 120} |        13       |
|  0.530341885869 | 0.0214106729429 | {'activation': 'logistic', 'hidden_layer_sizes': 150} |        13       |
|  0.529163036618 | 0.0225787161049 |    {'activation': 'tanh', 'hidden_layer_sizes': 20}   |        21       |
|  0.532092540732 | 0.0316324661633 |    {'activation': 'tanh', 'hidden_layer_sizes': 30}   |        12       |
|  0.541847465435 | 0.0453761243461 |    {'activation': 'tanh', 'hidden_layer_sizes': 50}   |        4        |
|  0.532855142788 | 0.0484464512043 |    {'activation': 'tanh', 'hidden_layer_sizes': 75}   |        11       |
|  0.538709063344 | 0.0491765806087 |   {'activation': 'tanh', 'hidden_layer_sizes': 100}   |        8        |
|  0.541358389783 | 0.0489062409073 |   {'activation': 'tanh', 'hidden_layer_sizes': 120}   |        5        |
|  0.544787485389 | 0.0481933362672 |   {'activation': 'tanh', 'hidden_layer_sizes': 150}   |        2        |
|  0.530341885869 | 0.0214106729429 |    {'activation': 'relu', 'hidden_layer_sizes': 20}   |        13       |
|  0.53445712155  | 0.0304493728824 |    {'activation': 'relu', 'hidden_layer_sizes': 30}   |        10       |
|  0.541338315829 | 0.0416934598373 |    {'activation': 'relu', 'hidden_layer_sizes': 50}   |        6        |
|  0.541894419147 | 0.0458406638786 |    {'activation': 'relu', 'hidden_layer_sizes': 75}   |        3        |
|  0.546660879375 | 0.0442288750271 |   {'activation': 'relu', 'hidden_layer_sizes': 100}   |        1        |
|  0.535111244159 | 0.0479969338439 |   {'activation': 'relu', 'hidden_layer_sizes': 120}   |        9        |
|  0.539593363301 | 0.0532749250581 |   {'activation': 'relu', 'hidden_layer_sizes': 150}   |        7        |
+-----------------+-----------------+-------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'activation': 'relu', 'hidden_layer_sizes': 100}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.70      1.00      0.83        97
          1       0.00      0.00      0.00        36
          2       0.00      0.00      0.00         4
          3       0.00      0.00      0.00         1

avg / total       0.49      0.70      0.58       138


Average accuracy on test set (using best parameters): 0.70

===================================================================
[ 0.70289855  0.          0.          0.        ]
===================================================================
Evaluating SVC
# Tuning hyper-parameters for f1_weighted

Fitting 10 folds for each of 128 candidates, totalling 1280 fits
Grid scores on validation set:

+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
| test_mean_score |  test_std_score |                                    params                                    | test_rank_score |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
|  0.511797942461 | 0.0222901176301 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.001}               |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.001}                 |        48       |
|  0.511797942461 | 0.0222901176301 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 0.01}                |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.01}                 |        48       |
|  0.511797942461 | 0.0222901176301 |        {'kernel': 'linear', 'C': 0.001, 'gamma': 0.10000000000000001}        |        48       |
|  0.511797942461 | 0.0222901176301 |         {'kernel': 'rbf', 'C': 0.001, 'gamma': 0.10000000000000001}          |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'linear', 'C': 0.001, 'gamma': 1.0}                |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 1.0}                  |        48       |
|  0.511797942461 | 0.0222901176301 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 10.0}                |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 0.001, 'gamma': 10.0}                 |        48       |
|  0.511797942461 | 0.0222901176301 |               {'kernel': 'linear', 'C': 0.001, 'gamma': 100.0}               |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 100.0}                 |        48       |
|  0.511797942461 | 0.0222901176301 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 1000.0}               |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'rbf', 'C': 0.001, 'gamma': 1000.0}                |        48       |
|  0.511797942461 | 0.0222901176301 |              {'kernel': 'linear', 'C': 0.001, 'gamma': 10000.0}              |        48       |
|  0.511797942461 | 0.0222901176301 |               {'kernel': 'rbf', 'C': 0.001, 'gamma': 10000.0}                |        48       |
|  0.511797942461 | 0.0222901176301 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 0.001}                |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.001}                 |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 0.01}                |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.01}                  |        48       |
|  0.511797942461 | 0.0222901176301 |        {'kernel': 'linear', 'C': 0.01, 'gamma': 0.10000000000000001}         |        48       |
|  0.511797942461 | 0.0222901176301 |          {'kernel': 'rbf', 'C': 0.01, 'gamma': 0.10000000000000001}          |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 1.0}                 |        48       |
|  0.511797942461 | 0.0222901176301 |                  {'kernel': 'rbf', 'C': 0.01, 'gamma': 1.0}                  |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'linear', 'C': 0.01, 'gamma': 10.0}                |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 10.0}                  |        48       |
|  0.511797942461 | 0.0222901176301 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 100.0}                |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 0.01, 'gamma': 100.0}                 |        48       |
|  0.511797942461 | 0.0222901176301 |               {'kernel': 'linear', 'C': 0.01, 'gamma': 1000.0}               |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 1000.0}                 |        48       |
|  0.511797942461 | 0.0222901176301 |              {'kernel': 'linear', 'C': 0.01, 'gamma': 10000.0}               |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'rbf', 'C': 0.01, 'gamma': 10000.0}                |        48       |
|  0.511797942461 | 0.0222901176301 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.001}        |        48       |
|  0.511797942461 | 0.0222901176301 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.001}          |        48       |
|  0.511797942461 | 0.0222901176301 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.01}         |        48       |
|  0.511797942461 | 0.0222901176301 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.01}          |        48       |
|  0.511797942461 | 0.0222901176301 | {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001} |        48       |
|  0.511797942461 | 0.0222901176301 |  {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 0.10000000000000001}   |        48       |
|  0.511797942461 | 0.0222901176301 |         {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1.0}         |        48       |
|  0.511797942461 | 0.0222901176301 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1.0}           |        48       |
|  0.511797942461 | 0.0222901176301 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10.0}         |        48       |
|  0.511797942461 | 0.0222901176301 |          {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10.0}          |        48       |
|  0.511797942461 | 0.0222901176301 |        {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 100.0}        |        48       |
|  0.511797942461 | 0.0222901176301 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 100.0}          |        48       |
|  0.511797942461 | 0.0222901176301 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 1000.0}        |        48       |
|  0.511797942461 | 0.0222901176301 |         {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 1000.0}         |        48       |
|  0.511797942461 | 0.0222901176301 |       {'kernel': 'linear', 'C': 0.10000000000000001, 'gamma': 10000.0}       |        48       |
|  0.511797942461 | 0.0222901176301 |        {'kernel': 'rbf', 'C': 0.10000000000000001, 'gamma': 10000.0}         |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.001}                |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.001}                  |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 0.01}                 |        48       |
|  0.511797942461 | 0.0222901176301 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.01}                  |        48       |
|  0.511797942461 | 0.0222901176301 |         {'kernel': 'linear', 'C': 1.0, 'gamma': 0.10000000000000001}         |        48       |
|  0.511797942461 | 0.0222901176301 |          {'kernel': 'rbf', 'C': 1.0, 'gamma': 0.10000000000000001}           |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'linear', 'C': 1.0, 'gamma': 1.0}                 |        48       |
|  0.511797942461 | 0.0222901176301 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 1.0}                   |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 10.0}                 |        48       |
|  0.511797942461 | 0.0222901176301 |                  {'kernel': 'rbf', 'C': 1.0, 'gamma': 10.0}                  |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'linear', 'C': 1.0, 'gamma': 100.0}                |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 100.0}                  |        48       |
|  0.511797942461 | 0.0222901176301 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 1000.0}                |        48       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 1.0, 'gamma': 1000.0}                 |        48       |
|  0.511797942461 | 0.0222901176301 |               {'kernel': 'linear', 'C': 1.0, 'gamma': 10000.0}               |        48       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'rbf', 'C': 1.0, 'gamma': 10000.0}                 |        48       |
|   0.5223336117  | 0.0297189361402 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 0.001}                |        29       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.001}                 |        48       |
|   0.5223336117  | 0.0297189361402 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 0.01}                |        29       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.01}                  |        48       |
|   0.5223336117  | 0.0297189361402 |        {'kernel': 'linear', 'C': 10.0, 'gamma': 0.10000000000000001}         |        29       |
|  0.511797942461 | 0.0222901176301 |          {'kernel': 'rbf', 'C': 10.0, 'gamma': 0.10000000000000001}          |        48       |
|   0.5223336117  | 0.0297189361402 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 1.0}                 |        29       |
|  0.520780399056 | 0.0323348532618 |                  {'kernel': 'rbf', 'C': 10.0, 'gamma': 1.0}                  |        42       |
|   0.5223336117  | 0.0297189361402 |                {'kernel': 'linear', 'C': 10.0, 'gamma': 10.0}                |        29       |
|  0.51215787767  | 0.0527812503172 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 10.0}                  |        47       |
|   0.5223336117  | 0.0297189361402 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 100.0}                |        29       |
|  0.558750255826 | 0.0619628272124 |                 {'kernel': 'rbf', 'C': 10.0, 'gamma': 100.0}                 |        2        |
|   0.5223336117  | 0.0297189361402 |               {'kernel': 'linear', 'C': 10.0, 'gamma': 1000.0}               |        29       |
|  0.515864867549 | 0.0204890727997 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 1000.0}                 |        45       |
|   0.5223336117  | 0.0297189361402 |              {'kernel': 'linear', 'C': 10.0, 'gamma': 10000.0}               |        29       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'rbf', 'C': 10.0, 'gamma': 10000.0}                |        48       |
|  0.525014439579 | 0.0630578647674 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.001}               |        19       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.001}                 |        48       |
|  0.525014439579 | 0.0630578647674 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 0.01}                |        19       |
|  0.511797942461 | 0.0222901176301 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.01}                 |        48       |
|  0.525014439579 | 0.0630578647674 |        {'kernel': 'linear', 'C': 100.0, 'gamma': 0.10000000000000001}        |        19       |
|  0.520763964364 | 0.0305347446767 |         {'kernel': 'rbf', 'C': 100.0, 'gamma': 0.10000000000000001}          |        43       |
|  0.525014439579 | 0.0630578647674 |                {'kernel': 'linear', 'C': 100.0, 'gamma': 1.0}                |        19       |
|  0.547942465427 | 0.0686480921661 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 1.0}                  |        6        |
|  0.525014439579 | 0.0630578647674 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 10.0}                |        19       |
|  0.54488095976  | 0.0532212658972 |                 {'kernel': 'rbf', 'C': 100.0, 'gamma': 10.0}                 |        8        |
|  0.525014439579 | 0.0630578647674 |               {'kernel': 'linear', 'C': 100.0, 'gamma': 100.0}               |        19       |
|  0.556433971843 | 0.0589555949613 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 100.0}                 |        4        |
|  0.525014439579 | 0.0630578647674 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 1000.0}               |        19       |
|  0.510807916174 | 0.0205785975278 |                {'kernel': 'rbf', 'C': 100.0, 'gamma': 1000.0}                |       119       |
|  0.525014439579 | 0.0630578647674 |              {'kernel': 'linear', 'C': 100.0, 'gamma': 10000.0}              |        19       |
|  0.520849913739 | 0.0353355918823 |               {'kernel': 'rbf', 'C': 100.0, 'gamma': 10000.0}                |        39       |
|   0.5327998711  | 0.0436515863998 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.001}               |        10       |
|  0.511797942461 | 0.0222901176301 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.001}                |        48       |
|   0.5327998711  | 0.0436515863998 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.01}               |        10       |
|  0.521940061588 |  0.029595875591 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.01}                 |        37       |
|   0.5327998711  | 0.0436515863998 |       {'kernel': 'linear', 'C': 1000.0, 'gamma': 0.10000000000000001}        |        10       |
|  0.557130414253 | 0.0607916368257 |         {'kernel': 'rbf', 'C': 1000.0, 'gamma': 0.10000000000000001}         |        3        |
|   0.5327998711  | 0.0436515863998 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 1.0}                |        10       |
|  0.505644945174 |  0.089456943093 |                 {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1.0}                 |       128       |
|   0.5327998711  | 0.0436515863998 |               {'kernel': 'linear', 'C': 1000.0, 'gamma': 10.0}               |        10       |
|  0.547001416521 | 0.0684507691817 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10.0}                 |        7        |
|   0.5327998711  | 0.0436515863998 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 100.0}               |        10       |
|  0.54866414293  | 0.0459262157609 |                {'kernel': 'rbf', 'C': 1000.0, 'gamma': 100.0}                |        5        |
|   0.5327998711  | 0.0436515863998 |              {'kernel': 'linear', 'C': 1000.0, 'gamma': 1000.0}              |        10       |
|  0.523420019268 | 0.0322319266362 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 1000.0}                |        27       |
|   0.5327998711  | 0.0436515863998 |             {'kernel': 'linear', 'C': 1000.0, 'gamma': 10000.0}              |        10       |
|  0.520849913739 | 0.0353355918823 |               {'kernel': 'rbf', 'C': 1000.0, 'gamma': 10000.0}               |        39       |
|  0.509263460814 | 0.0828575833519 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.001}              |       120       |
|  0.521940061588 |  0.029595875591 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.001}                |        37       |
|  0.509263460814 | 0.0828575833519 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.01}               |       120       |
|  0.539190388765 | 0.0407064197861 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.01}                |        9        |
|  0.509263460814 | 0.0828575833519 |       {'kernel': 'linear', 'C': 10000.0, 'gamma': 0.10000000000000001}       |       120       |
|  0.512873997496 | 0.0713457284624 |        {'kernel': 'rbf', 'C': 10000.0, 'gamma': 0.10000000000000001}         |        46       |
|  0.509263460814 | 0.0828575833519 |               {'kernel': 'linear', 'C': 10000.0, 'gamma': 1.0}               |       120       |
|  0.517745905207 | 0.0930695832249 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1.0}                 |        44       |
|  0.509263460814 | 0.0828575833519 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 10.0}               |       120       |
|  0.528271098466 | 0.0726481535559 |                {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10.0}                |        18       |
|  0.509263460814 | 0.0828575833519 |              {'kernel': 'linear', 'C': 10000.0, 'gamma': 100.0}              |       120       |
|  0.561934324145 | 0.0649185112556 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 100.0}                |        1        |
|  0.509263460814 | 0.0828575833519 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 1000.0}              |       120       |
|  0.523420019268 | 0.0322319266362 |               {'kernel': 'rbf', 'C': 10000.0, 'gamma': 1000.0}               |        27       |
|  0.509263460814 | 0.0828575833519 |             {'kernel': 'linear', 'C': 10000.0, 'gamma': 10000.0}             |       120       |
|  0.520849913739 | 0.0353355918823 |              {'kernel': 'rbf', 'C': 10000.0, 'gamma': 10000.0}               |        39       |
+-----------------+-----------------+------------------------------------------------------------------------------+-----------------+
Best parameters set found on validation set:

{'kernel': 'rbf', 'C': 10000.0, 'gamma': 100.0}


Scores on test set (using best parameters):

             precision    recall  f1-score   support

          0       0.77      0.90      0.83       103
          1       0.35      0.19      0.25        31
          2       0.00      0.00      0.00         3
          3       0.00      0.00      0.00         1

avg / total       0.65      0.72      0.68       138


Average accuracy on test set (using best parameters): 0.72

===================================================================
[ 0.76859504  0.35294118  0.          0.        ]
===================================================================
